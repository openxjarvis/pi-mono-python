"""
Auto-generated model definitions â€” mirrors TypeScript models.generated.ts

DO NOT EDIT MANUALLY.
Source: /Users/long/Desktop/XJarvis/pi-mono/packages/ai/src/models.generated.ts
"""
from __future__ import annotations

from .types import Model, ModelCost

MODELS: dict[str, Model] = {
    "amazon-bedrock/amazon.nova-2-lite-v1:0": Model(
        id='amazon.nova-2-lite-v1:0',
        name='Nova 2 Lite',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.33, output=2.75, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/amazon.nova-lite-v1:0": Model(
        id='amazon.nova-lite-v1:0',
        name='Nova Lite',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.06, output=0.24, cache_read=0.015, cache_write=0.0),
        context_window=300000,
        max_tokens=8192,
    ),
    "amazon-bedrock/amazon.nova-micro-v1:0": Model(
        id='amazon.nova-micro-v1:0',
        name='Nova Micro',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.035, output=0.14, cache_read=0.00875, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "amazon-bedrock/amazon.nova-premier-v1:0": Model(
        id='amazon.nova-premier-v1:0',
        name='Nova Premier',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=12.5, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=16384,
    ),
    "amazon-bedrock/amazon.nova-pro-v1:0": Model(
        id='amazon.nova-pro-v1:0',
        name='Nova Pro',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.8, output=3.2, cache_read=0.2, cache_write=0.0),
        context_window=300000,
        max_tokens=8192,
    ),
    "amazon-bedrock/amazon.titan-text-express-v1": Model(
        id='amazon.titan-text-express-v1',
        name='Titan Text G1 - Express',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.2, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/amazon.titan-text-express-v1:0:8k": Model(
        id='amazon.titan-text-express-v1:0:8k',
        name='Titan Text G1 - Express',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.2, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/anthropic.claude-3-5-haiku-20241022-v1:0": Model(
        id='anthropic.claude-3-5-haiku-20241022-v1:0',
        name='Claude Haiku 3.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.8, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "amazon-bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0": Model(
        id='anthropic.claude-3-5-sonnet-20240620-v1:0',
        name='Claude Sonnet 3.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "amazon-bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0": Model(
        id='anthropic.claude-3-5-sonnet-20241022-v2:0',
        name='Claude Sonnet 3.5 v2',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "amazon-bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0": Model(
        id='anthropic.claude-3-7-sonnet-20250219-v1:0',
        name='Claude Sonnet 3.7',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "amazon-bedrock/anthropic.claude-3-haiku-20240307-v1:0": Model(
        id='anthropic.claude-3-haiku-20240307-v1:0',
        name='Claude Haiku 3',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=1.25, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=4096,
    ),
    "amazon-bedrock/anthropic.claude-3-opus-20240229-v1:0": Model(
        id='anthropic.claude-3-opus-20240229-v1:0',
        name='Claude Opus 3',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=4096,
    ),
    "amazon-bedrock/anthropic.claude-3-sonnet-20240229-v1:0": Model(
        id='anthropic.claude-3-sonnet-20240229-v1:0',
        name='Claude Sonnet 3',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=4096,
    ),
    "amazon-bedrock/anthropic.claude-haiku-4-5-20251001-v1:0": Model(
        id='anthropic.claude-haiku-4-5-20251001-v1:0',
        name='Claude Haiku 4.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/anthropic.claude-opus-4-1-20250805-v1:0": Model(
        id='anthropic.claude-opus-4-1-20250805-v1:0',
        name='Claude Opus 4.1',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "amazon-bedrock/anthropic.claude-opus-4-20250514-v1:0": Model(
        id='anthropic.claude-opus-4-20250514-v1:0',
        name='Claude Opus 4',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "amazon-bedrock/anthropic.claude-opus-4-5-20251101-v1:0": Model(
        id='anthropic.claude-opus-4-5-20251101-v1:0',
        name='Claude Opus 4.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/anthropic.claude-opus-4-6-v1": Model(
        id='anthropic.claude-opus-4-6-v1',
        name='Claude Opus 4.6',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "amazon-bedrock/anthropic.claude-sonnet-4-20250514-v1:0": Model(
        id='anthropic.claude-sonnet-4-20250514-v1:0',
        name='Claude Sonnet 4',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0": Model(
        id='anthropic.claude-sonnet-4-5-20250929-v1:0',
        name='Claude Sonnet 4.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/cohere.command-r-plus-v1:0": Model(
        id='cohere.command-r-plus-v1:0',
        name='Command R+',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/cohere.command-r-v1:0": Model(
        id='cohere.command-r-v1:0',
        name='Command R',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/deepseek.r1-v1:0": Model(
        id='deepseek.r1-v1:0',
        name='DeepSeek-R1',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.35, output=5.4, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32768,
    ),
    "amazon-bedrock/deepseek.v3-v1:0": Model(
        id='deepseek.v3-v1:0',
        name='DeepSeek-V3.1',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.58, output=1.68, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=81920,
    ),
    "amazon-bedrock/deepseek.v3.2-v1:0": Model(
        id='deepseek.v3.2-v1:0',
        name='DeepSeek-V3.2',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.62, output=1.85, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=81920,
    ),
    "amazon-bedrock/eu.anthropic.claude-haiku-4-5-20251001-v1:0": Model(
        id='eu.anthropic.claude-haiku-4-5-20251001-v1:0',
        name='Claude Haiku 4.5 (EU)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/eu.anthropic.claude-opus-4-5-20251101-v1:0": Model(
        id='eu.anthropic.claude-opus-4-5-20251101-v1:0',
        name='Claude Opus 4.5 (EU)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/eu.anthropic.claude-opus-4-6-v1": Model(
        id='eu.anthropic.claude-opus-4-6-v1',
        name='Claude Opus 4.6 (EU)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "amazon-bedrock/eu.anthropic.claude-sonnet-4-20250514-v1:0": Model(
        id='eu.anthropic.claude-sonnet-4-20250514-v1:0',
        name='Claude Sonnet 4 (EU)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/eu.anthropic.claude-sonnet-4-5-20250929-v1:0": Model(
        id='eu.anthropic.claude-sonnet-4-5-20250929-v1:0',
        name='Claude Sonnet 4.5 (EU)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/global.anthropic.claude-haiku-4-5-20251001-v1:0": Model(
        id='global.anthropic.claude-haiku-4-5-20251001-v1:0',
        name='Claude Haiku 4.5 (Global)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/global.anthropic.claude-opus-4-5-20251101-v1:0": Model(
        id='global.anthropic.claude-opus-4-5-20251101-v1:0',
        name='Claude Opus 4.5 (Global)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/global.anthropic.claude-opus-4-6-v1": Model(
        id='global.anthropic.claude-opus-4-6-v1',
        name='Claude Opus 4.6 (Global)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "amazon-bedrock/global.anthropic.claude-sonnet-4-20250514-v1:0": Model(
        id='global.anthropic.claude-sonnet-4-20250514-v1:0',
        name='Claude Sonnet 4 (Global)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/global.anthropic.claude-sonnet-4-5-20250929-v1:0": Model(
        id='global.anthropic.claude-sonnet-4-5-20250929-v1:0',
        name='Claude Sonnet 4.5 (Global)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/google.gemma-3-27b-it": Model(
        id='google.gemma-3-27b-it',
        name='Google Gemma 3 27B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.12, output=0.2, cache_read=0.0, cache_write=0.0),
        context_window=202752,
        max_tokens=8192,
    ),
    "amazon-bedrock/google.gemma-3-4b-it": Model(
        id='google.gemma-3-4b-it',
        name='Gemma 3 4B IT',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.04, output=0.08, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-1-70b-instruct-v1:0": Model(
        id='meta.llama3-1-70b-instruct-v1:0',
        name='Llama 3.1 70B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.72, output=0.72, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-1-8b-instruct-v1:0": Model(
        id='meta.llama3-1-8b-instruct-v1:0',
        name='Llama 3.1 8B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.22, output=0.22, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-2-11b-instruct-v1:0": Model(
        id='meta.llama3-2-11b-instruct-v1:0',
        name='Llama 3.2 11B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.16, output=0.16, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-2-1b-instruct-v1:0": Model(
        id='meta.llama3-2-1b-instruct-v1:0',
        name='Llama 3.2 1B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.1, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-2-3b-instruct-v1:0": Model(
        id='meta.llama3-2-3b-instruct-v1:0',
        name='Llama 3.2 3B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-2-90b-instruct-v1:0": Model(
        id='meta.llama3-2-90b-instruct-v1:0',
        name='Llama 3.2 90B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.72, output=0.72, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama3-3-70b-instruct-v1:0": Model(
        id='meta.llama3-3-70b-instruct-v1:0',
        name='Llama 3.3 70B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.72, output=0.72, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/meta.llama4-maverick-17b-instruct-v1:0": Model(
        id='meta.llama4-maverick-17b-instruct-v1:0',
        name='Llama 4 Maverick 17B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.24, output=0.97, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=16384,
    ),
    "amazon-bedrock/meta.llama4-scout-17b-instruct-v1:0": Model(
        id='meta.llama4-scout-17b-instruct-v1:0',
        name='Llama 4 Scout 17B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.17, output=0.66, cache_read=0.0, cache_write=0.0),
        context_window=3500000,
        max_tokens=16384,
    ),
    "amazon-bedrock/minimax.minimax-m2": Model(
        id='minimax.minimax-m2',
        name='MiniMax M2',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=204608,
        max_tokens=128000,
    ),
    "amazon-bedrock/minimax.minimax-m2.1": Model(
        id='minimax.minimax-m2.1',
        name='MiniMax M2.1',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "amazon-bedrock/mistral.ministral-3-14b-instruct": Model(
        id='mistral.ministral-3-14b-instruct',
        name='Ministral 14B 3.0',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.2, output=0.2, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/mistral.ministral-3-8b-instruct": Model(
        id='mistral.ministral-3-8b-instruct',
        name='Ministral 3 8B',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/mistral.mistral-large-2402-v1:0": Model(
        id='mistral.mistral-large-2402-v1:0',
        name='Mistral Large (24.02)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/mistral.voxtral-mini-3b-2507": Model(
        id='mistral.voxtral-mini-3b-2507',
        name='Voxtral Mini 3B 2507',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.04, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/mistral.voxtral-small-24b-2507": Model(
        id='mistral.voxtral-small-24b-2507',
        name='Voxtral Small 24B 2507',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.35, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=8192,
    ),
    "amazon-bedrock/moonshot.kimi-k2-thinking": Model(
        id='moonshot.kimi-k2-thinking',
        name='Kimi K2 Thinking',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.5, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "amazon-bedrock/moonshotai.kimi-k2.5": Model(
        id='moonshotai.kimi-k2.5',
        name='Kimi K2.5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "amazon-bedrock/nvidia.nemotron-nano-12b-v2": Model(
        id='nvidia.nemotron-nano-12b-v2',
        name='NVIDIA Nemotron Nano 12B v2 VL BF16',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/nvidia.nemotron-nano-9b-v2": Model(
        id='nvidia.nemotron-nano-9b-v2',
        name='NVIDIA Nemotron Nano 9B v2',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.06, output=0.23, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/openai.gpt-oss-120b-1:0": Model(
        id='openai.gpt-oss-120b-1:0',
        name='gpt-oss-120b',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/openai.gpt-oss-20b-1:0": Model(
        id='openai.gpt-oss-20b-1:0',
        name='gpt-oss-20b',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/openai.gpt-oss-safeguard-120b": Model(
        id='openai.gpt-oss-safeguard-120b',
        name='GPT OSS Safeguard 120B',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/openai.gpt-oss-safeguard-20b": Model(
        id='openai.gpt-oss-safeguard-20b',
        name='GPT OSS Safeguard 20B',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.2, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "amazon-bedrock/qwen.qwen3-235b-a22b-2507-v1:0": Model(
        id='qwen.qwen3-235b-a22b-2507-v1:0',
        name='Qwen3 235B A22B 2507',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.22, output=0.88, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=131072,
    ),
    "amazon-bedrock/qwen.qwen3-32b-v1:0": Model(
        id='qwen.qwen3-32b-v1:0',
        name='Qwen3 32B (dense)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=16384,
        max_tokens=16384,
    ),
    "amazon-bedrock/qwen.qwen3-coder-30b-a3b-v1:0": Model(
        id='qwen.qwen3-coder-30b-a3b-v1:0',
        name='Qwen3 Coder 30B A3B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=131072,
    ),
    "amazon-bedrock/qwen.qwen3-coder-480b-a35b-v1:0": Model(
        id='qwen.qwen3-coder-480b-a35b-v1:0',
        name='Qwen3 Coder 480B A35B Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.22, output=1.8, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "amazon-bedrock/qwen.qwen3-next-80b-a3b": Model(
        id='qwen.qwen3-next-80b-a3b',
        name='Qwen/Qwen3-Next-80B-A3B-Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.14, output=1.4, cache_read=0.0, cache_write=0.0),
        context_window=262000,
        max_tokens=262000,
    ),
    "amazon-bedrock/qwen.qwen3-vl-235b-a22b": Model(
        id='qwen.qwen3-vl-235b-a22b',
        name='Qwen/Qwen3-VL-235B-A22B-Instruct',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=262000,
        max_tokens=262000,
    ),
    "amazon-bedrock/us.anthropic.claude-haiku-4-5-20251001-v1:0": Model(
        id='us.anthropic.claude-haiku-4-5-20251001-v1:0',
        name='Claude Haiku 4.5 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/us.anthropic.claude-opus-4-1-20250805-v1:0": Model(
        id='us.anthropic.claude-opus-4-1-20250805-v1:0',
        name='Claude Opus 4.1 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "amazon-bedrock/us.anthropic.claude-opus-4-20250514-v1:0": Model(
        id='us.anthropic.claude-opus-4-20250514-v1:0',
        name='Claude Opus 4 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "amazon-bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0": Model(
        id='us.anthropic.claude-opus-4-5-20251101-v1:0',
        name='Claude Opus 4.5 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/us.anthropic.claude-opus-4-6-v1": Model(
        id='us.anthropic.claude-opus-4-6-v1',
        name='Claude Opus 4.6 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "amazon-bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0": Model(
        id='us.anthropic.claude-sonnet-4-20250514-v1:0',
        name='Claude Sonnet 4 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0": Model(
        id='us.anthropic.claude-sonnet-4-5-20250929-v1:0',
        name='Claude Sonnet 4.5 (US)',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "amazon-bedrock/writer.palmyra-x4-v1:0": Model(
        id='writer.palmyra-x4-v1:0',
        name='Palmyra X4',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=122880,
        max_tokens=8192,
    ),
    "amazon-bedrock/writer.palmyra-x5-v1:0": Model(
        id='writer.palmyra-x5-v1:0',
        name='Palmyra X5',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=1040000,
        max_tokens=8192,
    ),
    "amazon-bedrock/zai.glm-4.7": Model(
        id='zai.glm-4.7',
        name='GLM-4.7',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "amazon-bedrock/zai.glm-4.7-flash": Model(
        id='zai.glm-4.7-flash',
        name='GLM-4.7-Flash',
        api='bedrock-converse-stream',
        provider='amazon-bedrock',
        base_url='https://bedrock-runtime.us-east-1.amazonaws.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.4, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=131072,
    ),
    "anthropic/claude-3-5-haiku-20241022": Model(
        id='claude-3-5-haiku-20241022',
        name='Claude Haiku 3.5',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.8, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "anthropic/claude-3-5-haiku-latest": Model(
        id='claude-3-5-haiku-latest',
        name='Claude Haiku 3.5 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.8, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "anthropic/claude-3-5-sonnet-20240620": Model(
        id='claude-3-5-sonnet-20240620',
        name='Claude Sonnet 3.5',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "anthropic/claude-3-5-sonnet-20241022": Model(
        id='claude-3-5-sonnet-20241022',
        name='Claude Sonnet 3.5 v2',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "anthropic/claude-3-7-sonnet-20250219": Model(
        id='claude-3-7-sonnet-20250219',
        name='Claude Sonnet 3.7',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-3-7-sonnet-latest": Model(
        id='claude-3-7-sonnet-latest',
        name='Claude Sonnet 3.7 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-3-haiku-20240307": Model(
        id='claude-3-haiku-20240307',
        name='Claude Haiku 3',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=1.25, cache_read=0.03, cache_write=0.3),
        context_window=200000,
        max_tokens=4096,
    ),
    "anthropic/claude-3-opus-20240229": Model(
        id='claude-3-opus-20240229',
        name='Claude Opus 3',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=4096,
    ),
    "anthropic/claude-3-sonnet-20240229": Model(
        id='claude-3-sonnet-20240229',
        name='Claude Sonnet 3',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=0.3),
        context_window=200000,
        max_tokens=4096,
    ),
    "anthropic/claude-haiku-4-5": Model(
        id='claude-haiku-4-5',
        name='Claude Haiku 4.5 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-haiku-4-5-20251001": Model(
        id='claude-haiku-4-5-20251001',
        name='Claude Haiku 4.5',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-opus-4-0": Model(
        id='claude-opus-4-0',
        name='Claude Opus 4 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "anthropic/claude-opus-4-1": Model(
        id='claude-opus-4-1',
        name='Claude Opus 4.1 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "anthropic/claude-opus-4-1-20250805": Model(
        id='claude-opus-4-1-20250805',
        name='Claude Opus 4.1',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "anthropic/claude-opus-4-20250514": Model(
        id='claude-opus-4-20250514',
        name='Claude Opus 4',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "anthropic/claude-opus-4-5": Model(
        id='claude-opus-4-5',
        name='Claude Opus 4.5 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-opus-4-5-20251101": Model(
        id='claude-opus-4-5-20251101',
        name='Claude Opus 4.5',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-opus-4-6": Model(
        id='claude-opus-4-6',
        name='Claude Opus 4.6',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "anthropic/claude-sonnet-4-0": Model(
        id='claude-sonnet-4-0',
        name='Claude Sonnet 4 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-sonnet-4-20250514": Model(
        id='claude-sonnet-4-20250514',
        name='Claude Sonnet 4',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-sonnet-4-5": Model(
        id='claude-sonnet-4-5',
        name='Claude Sonnet 4.5 (latest)',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-sonnet-4-5-20250929": Model(
        id='claude-sonnet-4-5-20250929',
        name='Claude Sonnet 4.5',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "anthropic/claude-sonnet-4-6": Model(
        id='claude-sonnet-4-6',
        name='Claude Sonnet 4.6',
        api='anthropic-messages',
        provider='anthropic',
        base_url='https://api.anthropic.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "azure-openai-responses/codex-mini-latest": Model(
        id='codex-mini-latest',
        name='Codex Mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.5, output=6.0, cache_read=0.375, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/gpt-4": Model(
        id='gpt-4',
        name='GPT-4',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=30.0, output=60.0, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "azure-openai-responses/gpt-4-turbo": Model(
        id='gpt-4-turbo',
        name='GPT-4 Turbo',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "azure-openai-responses/gpt-4.1": Model(
        id='gpt-4.1',
        name='GPT-4.1',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "azure-openai-responses/gpt-4.1-mini": Model(
        id='gpt-4.1-mini',
        name='GPT-4.1 mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.4, output=1.6, cache_read=0.1, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "azure-openai-responses/gpt-4.1-nano": Model(
        id='gpt-4.1-nano',
        name='GPT-4.1 nano',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.03, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "azure-openai-responses/gpt-4o": Model(
        id='gpt-4o',
        name='GPT-4o',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-4o-2024-05-13": Model(
        id='gpt-4o-2024-05-13',
        name='GPT-4o (2024-05-13)',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "azure-openai-responses/gpt-4o-2024-08-06": Model(
        id='gpt-4o-2024-08-06',
        name='GPT-4o (2024-08-06)',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-4o-2024-11-20": Model(
        id='gpt-4o-2024-11-20',
        name='GPT-4o (2024-11-20)',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-4o-mini": Model(
        id='gpt-4o-mini',
        name='GPT-4o mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.08, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-5": Model(
        id='gpt-5',
        name='GPT-5',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5-chat-latest": Model(
        id='gpt-5-chat-latest',
        name='GPT-5 Chat Latest',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-5-codex": Model(
        id='gpt-5-codex',
        name='GPT-5-Codex',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5-mini": Model(
        id='gpt-5-mini',
        name='GPT-5 Mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5-nano": Model(
        id='gpt-5-nano',
        name='GPT-5 Nano',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.05, output=0.4, cache_read=0.005, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5-pro": Model(
        id='gpt-5-pro',
        name='GPT-5 Pro',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=120.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=272000,
    ),
    "azure-openai-responses/gpt-5.1": Model(
        id='gpt-5.1',
        name='GPT-5.1',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.1-chat-latest": Model(
        id='gpt-5.1-chat-latest',
        name='GPT-5.1 Chat',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-5.1-codex": Model(
        id='gpt-5.1-codex',
        name='GPT-5.1 Codex',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.1-codex-max": Model(
        id='gpt-5.1-codex-max',
        name='GPT-5.1 Codex Max',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.1-codex-mini": Model(
        id='gpt-5.1-codex-mini',
        name='GPT-5.1 Codex mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.2": Model(
        id='gpt-5.2',
        name='GPT-5.2',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.2-chat-latest": Model(
        id='gpt-5.2-chat-latest',
        name='GPT-5.2 Chat',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "azure-openai-responses/gpt-5.2-codex": Model(
        id='gpt-5.2-codex',
        name='GPT-5.2 Codex',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.2-pro": Model(
        id='gpt-5.2-pro',
        name='GPT-5.2 Pro',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=21.0, output=168.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.3-codex": Model(
        id='gpt-5.3-codex',
        name='GPT-5.3 Codex',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "azure-openai-responses/gpt-5.3-codex-spark": Model(
        id='gpt-5.3-codex-spark',
        name='GPT-5.3 Codex Spark',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
    ),
    "azure-openai-responses/o1": Model(
        id='o1',
        name='o1',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=60.0, cache_read=7.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o1-pro": Model(
        id='o1-pro',
        name='o1-pro',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=150.0, output=600.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o3": Model(
        id='o3',
        name='o3',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o3-deep-research": Model(
        id='o3-deep-research',
        name='o3-deep-research',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=40.0, cache_read=2.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o3-mini": Model(
        id='o3-mini',
        name='o3-mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o3-pro": Model(
        id='o3-pro',
        name='o3-pro',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=20.0, output=80.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o4-mini": Model(
        id='o4-mini',
        name='o4-mini',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.28, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "azure-openai-responses/o4-mini-deep-research": Model(
        id='o4-mini-deep-research',
        name='o4-mini-deep-research',
        api='azure-openai-responses',
        provider='azure-openai-responses',
        base_url='',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "cerebras/gpt-oss-120b": Model(
        id='gpt-oss-120b',
        name='GPT OSS 120B',
        api='openai-completions',
        provider='cerebras',
        base_url='https://api.cerebras.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.25, output=0.69, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "cerebras/llama3.1-8b": Model(
        id='llama3.1-8b',
        name='Llama 3.1 8B',
        api='openai-completions',
        provider='cerebras',
        base_url='https://api.cerebras.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.1, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=8000,
    ),
    "cerebras/qwen-3-235b-a22b-instruct-2507": Model(
        id='qwen-3-235b-a22b-instruct-2507',
        name='Qwen 3 235B Instruct',
        api='openai-completions',
        provider='cerebras',
        base_url='https://api.cerebras.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.6, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=32000,
    ),
    "cerebras/zai-glm-4.7": Model(
        id='zai-glm-4.7',
        name='Z.AI GLM-4.7',
        api='openai-completions',
        provider='cerebras',
        base_url='https://api.cerebras.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.25, output=2.75, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=40000,
    ),
    "github-copilot/claude-haiku-4.5": Model(
        id='claude-haiku-4.5',
        name='Claude Haiku 4.5',
        api='anthropic-messages',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/claude-opus-4.5": Model(
        id='claude-opus-4.5',
        name='Claude Opus 4.5',
        api='anthropic-messages',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/claude-opus-4.6": Model(
        id='claude-opus-4.6',
        name='Claude Opus 4.6',
        api='anthropic-messages',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/claude-sonnet-4": Model(
        id='claude-sonnet-4',
        name='Claude Sonnet 4',
        api='anthropic-messages',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/claude-sonnet-4.5": Model(
        id='claude-sonnet-4.5',
        name='Claude Sonnet 4.5',
        api='anthropic-messages',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gemini-2.5-pro": Model(
        id='gemini-2.5-pro',
        name='Gemini 2.5 Pro',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "github-copilot/gemini-3-flash-preview": Model(
        id='gemini-3-flash-preview',
        name='Gemini 3 Flash',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "github-copilot/gemini-3-pro-preview": Model(
        id='gemini-3-pro-preview',
        name='Gemini 3 Pro Preview',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "github-copilot/gpt-4.1": Model(
        id='gpt-4.1',
        name='GPT-4.1',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=64000,
        max_tokens=16384,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "github-copilot/gpt-4o": Model(
        id='gpt-4o',
        name='GPT-4o',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=64000,
        max_tokens=16384,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "github-copilot/gpt-5": Model(
        id='gpt-5',
        name='GPT-5',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5-mini": Model(
        id='gpt-5-mini',
        name='GPT-5-mini',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.1": Model(
        id='gpt-5.1',
        name='GPT-5.1',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.1-codex": Model(
        id='gpt-5.1-codex',
        name='GPT-5.1-Codex',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.1-codex-max": Model(
        id='gpt-5.1-codex-max',
        name='GPT-5.1-Codex-max',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.1-codex-mini": Model(
        id='gpt-5.1-codex-mini',
        name='GPT-5.1-Codex-mini',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.2": Model(
        id='gpt-5.2',
        name='GPT-5.2',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/gpt-5.2-codex": Model(
        id='gpt-5.2-codex',
        name='GPT-5.2-Codex',
        api='openai-responses',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
    ),
    "github-copilot/grok-code-fast-1": Model(
        id='grok-code-fast-1',
        name='Grok Code Fast 1',
        api='openai-completions',
        provider='github-copilot',
        base_url='https://api.individual.githubcopilot.com',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
        headers={'User-Agent': 'GitHubCopilotChat/0.35.0', 'Editor-Version': 'vscode/1.107.0', 'Editor-Plugin-Version': 'copilot-chat/0.35.0', 'Copilot-Integration-Id': 'vscode-chat'},
        compat={'supportsStore': False, 'supportsDeveloperRole': False, 'supportsReasoningEffort': False},
    ),
    "google-antigravity/claude-opus-4-5-thinking": Model(
        id='claude-opus-4-5-thinking',
        name='Claude Opus 4.5 Thinking (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "google-antigravity/claude-sonnet-4-5": Model(
        id='claude-sonnet-4-5',
        name='Claude Sonnet 4.5 (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "google-antigravity/claude-sonnet-4-5-thinking": Model(
        id='claude-sonnet-4-5-thinking',
        name='Claude Sonnet 4.5 Thinking (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "google-antigravity/gemini-3-flash": Model(
        id='gemini-3-flash',
        name='Gemini 3 Flash (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.5, cache_write=0.0),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-antigravity/gemini-3-pro-high": Model(
        id='gemini-3-pro-high',
        name='Gemini 3 Pro High (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.2, cache_write=2.375),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-antigravity/gemini-3-pro-low": Model(
        id='gemini-3-pro-low',
        name='Gemini 3 Pro Low (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.2, cache_write=2.375),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-antigravity/gpt-oss-120b-medium": Model(
        id='gpt-oss-120b-medium',
        name='GPT-OSS 120B Medium (Antigravity)',
        api='google-gemini-cli',
        provider='google-antigravity',
        base_url='https://daily-cloudcode-pa.sandbox.googleapis.com',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09, output=0.36, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "google-gemini-cli/gemini-2.0-flash": Model(
        id='gemini-2.0-flash',
        name='Gemini 2.0 Flash (Cloud Code Assist)',
        api='google-gemini-cli',
        provider='google-gemini-cli',
        base_url='https://cloudcode-pa.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=8192,
    ),
    "google-gemini-cli/gemini-2.5-flash": Model(
        id='gemini-2.5-flash',
        name='Gemini 2.5 Flash (Cloud Code Assist)',
        api='google-gemini-cli',
        provider='google-gemini-cli',
        base_url='https://cloudcode-pa.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-gemini-cli/gemini-2.5-pro": Model(
        id='gemini-2.5-pro',
        name='Gemini 2.5 Pro (Cloud Code Assist)',
        api='google-gemini-cli',
        provider='google-gemini-cli',
        base_url='https://cloudcode-pa.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-gemini-cli/gemini-3-flash-preview": Model(
        id='gemini-3-flash-preview',
        name='Gemini 3 Flash Preview (Cloud Code Assist)',
        api='google-gemini-cli',
        provider='google-gemini-cli',
        base_url='https://cloudcode-pa.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-gemini-cli/gemini-3-pro-preview": Model(
        id='gemini-3-pro-preview',
        name='Gemini 3 Pro Preview (Cloud Code Assist)',
        api='google-gemini-cli',
        provider='google-gemini-cli',
        base_url='https://cloudcode-pa.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=65535,
    ),
    "google-vertex/gemini-1.5-flash": Model(
        id='gemini-1.5-flash',
        name='Gemini 1.5 Flash (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.01875, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google-vertex/gemini-1.5-flash-8b": Model(
        id='gemini-1.5-flash-8b',
        name='Gemini 1.5 Flash-8B (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0375, output=0.15, cache_read=0.01, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google-vertex/gemini-1.5-pro": Model(
        id='gemini-1.5-pro',
        name='Gemini 1.5 Pro (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=5.0, cache_read=0.3125, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google-vertex/gemini-2.0-flash": Model(
        id='gemini-2.0-flash',
        name='Gemini 2.0 Flash (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0375, cache_write=0.0),
        context_window=1048576,
        max_tokens=8192,
    ),
    "google-vertex/gemini-2.0-flash-lite": Model(
        id='gemini-2.0-flash-lite',
        name='Gemini 2.0 Flash Lite (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.01875, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-2.5-flash": Model(
        id='gemini-2.5-flash',
        name='Gemini 2.5 Flash (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.03, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-2.5-flash-lite": Model(
        id='gemini-2.5-flash-lite',
        name='Gemini 2.5 Flash Lite (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.01, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-2.5-flash-lite-preview-09-2025": Model(
        id='gemini-2.5-flash-lite-preview-09-2025',
        name='Gemini 2.5 Flash Lite Preview 09-25 (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.01, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-2.5-pro": Model(
        id='gemini-2.5-pro',
        name='Gemini 2.5 Pro (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-3-flash-preview": Model(
        id='gemini-3-flash-preview',
        name='Gemini 3 Flash Preview (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.05, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google-vertex/gemini-3-pro-preview": Model(
        id='gemini-3-pro-preview',
        name='Gemini 3 Pro Preview (Vertex)',
        api='google-vertex',
        provider='google-vertex',
        base_url='https://{location}-aiplatform.googleapis.com',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.2, cache_write=0.0),
        context_window=1000000,
        max_tokens=64000,
    ),
    "google/gemini-1.5-flash": Model(
        id='gemini-1.5-flash',
        name='Gemini 1.5 Flash',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.01875, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google/gemini-1.5-flash-8b": Model(
        id='gemini-1.5-flash-8b',
        name='Gemini 1.5 Flash-8B',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0375, output=0.15, cache_read=0.01, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google/gemini-1.5-pro": Model(
        id='gemini-1.5-pro',
        name='Gemini 1.5 Pro',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=5.0, cache_read=0.3125, cache_write=0.0),
        context_window=1000000,
        max_tokens=8192,
    ),
    "google/gemini-2.0-flash": Model(
        id='gemini-2.0-flash',
        name='Gemini 2.0 Flash',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0.0),
        context_window=1048576,
        max_tokens=8192,
    ),
    "google/gemini-2.0-flash-lite": Model(
        id='gemini-2.0-flash-lite',
        name='Gemini 2.0 Flash Lite',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=8192,
    ),
    "google/gemini-2.5-flash": Model(
        id='gemini-2.5-flash',
        name='Gemini 2.5 Flash',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-lite": Model(
        id='gemini-2.5-flash-lite',
        name='Gemini 2.5 Flash Lite',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-lite-preview-06-17": Model(
        id='gemini-2.5-flash-lite-preview-06-17',
        name='Gemini 2.5 Flash Lite Preview 06-17',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-lite-preview-09-2025": Model(
        id='gemini-2.5-flash-lite-preview-09-2025',
        name='Gemini 2.5 Flash Lite Preview 09-25',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-preview-04-17": Model(
        id='gemini-2.5-flash-preview-04-17',
        name='Gemini 2.5 Flash Preview 04-17',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0375, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-preview-05-20": Model(
        id='gemini-2.5-flash-preview-05-20',
        name='Gemini 2.5 Flash Preview 05-20',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0375, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-flash-preview-09-2025": Model(
        id='gemini-2.5-flash-preview-09-2025',
        name='Gemini 2.5 Flash Preview 09-25',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-pro": Model(
        id='gemini-2.5-pro',
        name='Gemini 2.5 Pro',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.31, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-pro-preview-05-06": Model(
        id='gemini-2.5-pro-preview-05-06',
        name='Gemini 2.5 Pro Preview 05-06',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.31, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-2.5-pro-preview-06-05": Model(
        id='gemini-2.5-pro-preview-06-05',
        name='Gemini 2.5 Pro Preview 06-05',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.31, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-3-flash-preview": Model(
        id='gemini-3-flash-preview',
        name='Gemini 3 Flash Preview',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.05, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-3-pro-preview": Model(
        id='gemini-3-pro-preview',
        name='Gemini 3 Pro Preview',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.2, cache_write=0.0),
        context_window=1000000,
        max_tokens=64000,
    ),
    "google/gemini-flash-latest": Model(
        id='gemini-flash-latest',
        name='Gemini Flash Latest',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.075, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-flash-lite-latest": Model(
        id='gemini-flash-lite-latest',
        name='Gemini Flash-Lite Latest',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.025, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "google/gemini-live-2.5-flash": Model(
        id='gemini-live-2.5-flash',
        name='Gemini Live 2.5 Flash',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8000,
    ),
    "google/gemini-live-2.5-flash-preview-native-audio": Model(
        id='gemini-live-2.5-flash-preview-native-audio',
        name='Gemini Live 2.5 Flash Preview Native Audio',
        api='google-generative-ai',
        provider='google',
        base_url='https://generativelanguage.googleapis.com/v1beta',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.5, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "groq/deepseek-r1-distill-llama-70b": Model(
        id='deepseek-r1-distill-llama-70b',
        name='DeepSeek R1 Distill Llama 70B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.75, output=0.99, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "groq/gemma2-9b-it": Model(
        id='gemma2-9b-it',
        name='Gemma 2 9B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.2, output=0.2, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "groq/llama-3.1-8b-instant": Model(
        id='llama-3.1-8b-instant',
        name='Llama 3.1 8B Instant',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.05, output=0.08, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "groq/llama-3.3-70b-versatile": Model(
        id='llama-3.3-70b-versatile',
        name='Llama 3.3 70B Versatile',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.59, output=0.79, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "groq/llama3-70b-8192": Model(
        id='llama3-70b-8192',
        name='Llama 3 70B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.59, output=0.79, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "groq/llama3-8b-8192": Model(
        id='llama3-8b-8192',
        name='Llama 3 8B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.05, output=0.08, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "groq/meta-llama/llama-4-maverick-17b-128e-instruct": Model(
        id='meta-llama/llama-4-maverick-17b-128e-instruct',
        name='Llama 4 Maverick 17B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "groq/meta-llama/llama-4-scout-17b-16e-instruct": Model(
        id='meta-llama/llama-4-scout-17b-16e-instruct',
        name='Llama 4 Scout 17B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.11, output=0.34, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "groq/mistral-saba-24b": Model(
        id='mistral-saba-24b',
        name='Mistral Saba 24B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.79, output=0.79, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "groq/moonshotai/kimi-k2-instruct": Model(
        id='moonshotai/kimi-k2-instruct',
        name='Kimi K2 Instruct',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "groq/moonshotai/kimi-k2-instruct-0905": Model(
        id='moonshotai/kimi-k2-instruct-0905',
        name='Kimi K2 Instruct 0905',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=16384,
    ),
    "groq/openai/gpt-oss-120b": Model(
        id='openai/gpt-oss-120b',
        name='GPT OSS 120B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "groq/openai/gpt-oss-20b": Model(
        id='openai/gpt-oss-20b',
        name='GPT OSS 20B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "groq/qwen-qwq-32b": Model(
        id='qwen-qwq-32b',
        name='Qwen QwQ 32B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.29, output=0.39, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "groq/qwen/qwen3-32b": Model(
        id='qwen/qwen3-32b',
        name='Qwen3 32B',
        api='openai-completions',
        provider='groq',
        base_url='https://api.groq.com/openai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.29, output=0.59, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "huggingface/MiniMaxAI/MiniMax-M2.1": Model(
        id='MiniMaxAI/MiniMax-M2.1',
        name='MiniMax-M2.1',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/MiniMaxAI/MiniMax-M2.5": Model(
        id='MiniMaxAI/MiniMax-M2.5',
        name='MiniMax-M2.5',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.03, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3-235B-A22B-Thinking-2507": Model(
        id='Qwen/Qwen3-235B-A22B-Thinking-2507',
        name='Qwen3-235B-A22B-Thinking-2507',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3-Coder-480B-A35B-Instruct": Model(
        id='Qwen/Qwen3-Coder-480B-A35B-Instruct',
        name='Qwen3-Coder-480B-A35B-Instruct',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=66536,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3-Coder-Next": Model(
        id='Qwen/Qwen3-Coder-Next',
        name='Qwen3-Coder-Next',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.2, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3-Next-80B-A3B-Instruct": Model(
        id='Qwen/Qwen3-Next-80B-A3B-Instruct',
        name='Qwen3-Next-80B-A3B-Instruct',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=66536,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3-Next-80B-A3B-Thinking": Model(
        id='Qwen/Qwen3-Next-80B-A3B-Thinking',
        name='Qwen3-Next-80B-A3B-Thinking',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/Qwen/Qwen3.5-397B-A17B": Model(
        id='Qwen/Qwen3.5-397B-A17B',
        name='Qwen3.5-397B-A17B',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=3.6, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/XiaomiMiMo/MiMo-V2-Flash": Model(
        id='XiaomiMiMo/MiMo-V2-Flash',
        name='MiMo-V2-Flash',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/deepseek-ai/DeepSeek-R1-0528": Model(
        id='deepseek-ai/DeepSeek-R1-0528',
        name='DeepSeek-R1-0528',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=3.0, output=5.0, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=163840,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/deepseek-ai/DeepSeek-V3.2": Model(
        id='deepseek-ai/DeepSeek-V3.2',
        name='DeepSeek-V3.2',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.28, output=0.4, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=65536,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/moonshotai/Kimi-K2-Instruct": Model(
        id='moonshotai/Kimi-K2-Instruct',
        name='Kimi-K2-Instruct',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/moonshotai/Kimi-K2-Instruct-0905": Model(
        id='moonshotai/Kimi-K2-Instruct-0905',
        name='Kimi-K2-Instruct-0905',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=16384,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/moonshotai/Kimi-K2-Thinking": Model(
        id='moonshotai/Kimi-K2-Thinking',
        name='Kimi-K2-Thinking',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.5, cache_read=0.15, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/moonshotai/Kimi-K2.5": Model(
        id='moonshotai/Kimi-K2.5',
        name='Kimi-K2.5',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=3.0, cache_read=0.1, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/zai-org/GLM-4.7": Model(
        id='zai-org/GLM-4.7',
        name='GLM-4.7',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/zai-org/GLM-4.7-Flash": Model(
        id='zai-org/GLM-4.7-Flash',
        name='GLM-4.7-Flash',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=128000,
        compat={'supportsDeveloperRole': False},
    ),
    "huggingface/zai-org/GLM-5": Model(
        id='zai-org/GLM-5',
        name='GLM-5',
        api='openai-completions',
        provider='huggingface',
        base_url='https://router.huggingface.co/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.2, cache_read=0.2, cache_write=0.0),
        context_window=202752,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False},
    ),
    "kimi-coding/k2p5": Model(
        id='k2p5',
        name='Kimi K2.5',
        api='anthropic-messages',
        provider='kimi-coding',
        base_url='https://api.kimi.com/coding',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
    ),
    "kimi-coding/kimi-k2-thinking": Model(
        id='kimi-k2-thinking',
        name='Kimi K2 Thinking',
        api='anthropic-messages',
        provider='kimi-coding',
        base_url='https://api.kimi.com/coding',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
    ),
    "minimax-cn/MiniMax-M2": Model(
        id='MiniMax-M2',
        name='MiniMax-M2',
        api='anthropic-messages',
        provider='minimax-cn',
        base_url='https://api.minimaxi.com/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=196608,
        max_tokens=128000,
    ),
    "minimax-cn/MiniMax-M2.1": Model(
        id='MiniMax-M2.1',
        name='MiniMax-M2.1',
        api='anthropic-messages',
        provider='minimax-cn',
        base_url='https://api.minimaxi.com/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "minimax-cn/MiniMax-M2.5": Model(
        id='MiniMax-M2.5',
        name='MiniMax-M2.5',
        api='anthropic-messages',
        provider='minimax-cn',
        base_url='https://api.minimaxi.com/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.03, cache_write=0.375),
        context_window=204800,
        max_tokens=131072,
    ),
    "minimax-cn/MiniMax-M2.5-highspeed": Model(
        id='MiniMax-M2.5-highspeed',
        name='MiniMax-M2.5-highspeed',
        api='anthropic-messages',
        provider='minimax-cn',
        base_url='https://api.minimaxi.com/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.4, cache_read=0.06, cache_write=0.375),
        context_window=204800,
        max_tokens=131072,
    ),
    "minimax/MiniMax-M2": Model(
        id='MiniMax-M2',
        name='MiniMax-M2',
        api='anthropic-messages',
        provider='minimax',
        base_url='https://api.minimax.io/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=196608,
        max_tokens=128000,
    ),
    "minimax/MiniMax-M2.1": Model(
        id='MiniMax-M2.1',
        name='MiniMax-M2.1',
        api='anthropic-messages',
        provider='minimax',
        base_url='https://api.minimax.io/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "minimax/MiniMax-M2.5": Model(
        id='MiniMax-M2.5',
        name='MiniMax-M2.5',
        api='anthropic-messages',
        provider='minimax',
        base_url='https://api.minimax.io/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.03, cache_write=0.375),
        context_window=204800,
        max_tokens=131072,
    ),
    "minimax/MiniMax-M2.5-highspeed": Model(
        id='MiniMax-M2.5-highspeed',
        name='MiniMax-M2.5-highspeed',
        api='anthropic-messages',
        provider='minimax',
        base_url='https://api.minimax.io/anthropic',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.4, cache_read=0.06, cache_write=0.375),
        context_window=204800,
        max_tokens=131072,
    ),
    "mistral/codestral-latest": Model(
        id='codestral-latest',
        name='Codestral',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.9, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=4096,
    ),
    "mistral/devstral-2512": Model(
        id='devstral-2512',
        name='Devstral 2',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "mistral/devstral-medium-2507": Model(
        id='devstral-medium-2507',
        name='Devstral Medium',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/devstral-medium-latest": Model(
        id='devstral-medium-latest',
        name='Devstral 2',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "mistral/devstral-small-2505": Model(
        id='devstral-small-2505',
        name='Devstral Small 2505',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/devstral-small-2507": Model(
        id='devstral-small-2507',
        name='Devstral Small',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/labs-devstral-small-2512": Model(
        id='labs-devstral-small-2512',
        name='Devstral Small 2',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "mistral/magistral-medium-latest": Model(
        id='magistral-medium-latest',
        name='Magistral Medium',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=2.0, output=5.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "mistral/magistral-small": Model(
        id='magistral-small',
        name='Magistral Small',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/ministral-3b-latest": Model(
        id='ministral-3b-latest',
        name='Ministral 3B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.04, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/ministral-8b-latest": Model(
        id='ministral-8b-latest',
        name='Ministral 8B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.1, output=0.1, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/mistral-large-2411": Model(
        id='mistral-large-2411',
        name='Mistral Large 2.1',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "mistral/mistral-large-2512": Model(
        id='mistral-large-2512',
        name='Mistral Large 3',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "mistral/mistral-large-latest": Model(
        id='mistral-large-latest',
        name='Mistral Large',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "mistral/mistral-medium-2505": Model(
        id='mistral-medium-2505',
        name='Mistral Medium 3',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "mistral/mistral-medium-2508": Model(
        id='mistral-medium-2508',
        name='Mistral Medium 3.1',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "mistral/mistral-medium-latest": Model(
        id='mistral-medium-latest',
        name='Mistral Medium',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.4, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "mistral/mistral-nemo": Model(
        id='mistral-nemo',
        name='Mistral Nemo',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/mistral-small-2506": Model(
        id='mistral-small-2506',
        name='Mistral Small 3.2',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "mistral/mistral-small-latest": Model(
        id='mistral-small-latest',
        name='Mistral Small',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "mistral/open-mistral-7b": Model(
        id='open-mistral-7b',
        name='Mistral 7B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=0.25, cache_read=0.0, cache_write=0.0),
        context_window=8000,
        max_tokens=8000,
    ),
    "mistral/open-mixtral-8x22b": Model(
        id='open-mixtral-8x22b',
        name='Mixtral 8x22B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=64000,
        max_tokens=64000,
    ),
    "mistral/open-mixtral-8x7b": Model(
        id='open-mixtral-8x7b',
        name='Mixtral 8x7B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.7, output=0.7, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=32000,
    ),
    "mistral/pixtral-12b": Model(
        id='pixtral-12b',
        name='Pixtral 12B',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "mistral/pixtral-large-latest": Model(
        id='pixtral-large-latest',
        name='Pixtral Large',
        api='openai-completions',
        provider='mistral',
        base_url='https://api.mistral.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.1": Model(
        id='gpt-5.1',
        name='GPT-5.1',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.1-codex-max": Model(
        id='gpt-5.1-codex-max',
        name='GPT-5.1 Codex Max',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.1-codex-mini": Model(
        id='gpt-5.1-codex-mini',
        name='GPT-5.1 Codex Mini',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.2": Model(
        id='gpt-5.2',
        name='GPT-5.2',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.2-codex": Model(
        id='gpt-5.2-codex',
        name='GPT-5.2 Codex',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.3-codex": Model(
        id='gpt-5.3-codex',
        name='GPT-5.3 Codex',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=272000,
        max_tokens=128000,
    ),
    "openai-codex/gpt-5.3-codex-spark": Model(
        id='gpt-5.3-codex-spark',
        name='GPT-5.3 Codex Spark',
        api='openai-codex-responses',
        provider='openai-codex',
        base_url='https://chatgpt.com/backend-api',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "openai/codex-mini-latest": Model(
        id='codex-mini-latest',
        name='Codex Mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.5, output=6.0, cache_read=0.375, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/gpt-4": Model(
        id='gpt-4',
        name='GPT-4',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=30.0, output=60.0, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "openai/gpt-4-turbo": Model(
        id='gpt-4-turbo',
        name='GPT-4 Turbo',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openai/gpt-4.1": Model(
        id='gpt-4.1',
        name='GPT-4.1',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openai/gpt-4.1-mini": Model(
        id='gpt-4.1-mini',
        name='GPT-4.1 mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.4, output=1.6, cache_read=0.1, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openai/gpt-4.1-nano": Model(
        id='gpt-4.1-nano',
        name='GPT-4.1 nano',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.1, output=0.4, cache_read=0.03, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openai/gpt-4o": Model(
        id='gpt-4o',
        name='GPT-4o',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-4o-2024-05-13": Model(
        id='gpt-4o-2024-05-13',
        name='GPT-4o (2024-05-13)',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openai/gpt-4o-2024-08-06": Model(
        id='gpt-4o-2024-08-06',
        name='GPT-4o (2024-08-06)',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-4o-2024-11-20": Model(
        id='gpt-4o-2024-11-20',
        name='GPT-4o (2024-11-20)',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-4o-mini": Model(
        id='gpt-4o-mini',
        name='GPT-4o mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.08, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-5": Model(
        id='gpt-5',
        name='GPT-5',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5-chat-latest": Model(
        id='gpt-5-chat-latest',
        name='GPT-5 Chat Latest',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-5-codex": Model(
        id='gpt-5-codex',
        name='GPT-5-Codex',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5-mini": Model(
        id='gpt-5-mini',
        name='GPT-5 Mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5-nano": Model(
        id='gpt-5-nano',
        name='GPT-5 Nano',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.05, output=0.4, cache_read=0.005, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5-pro": Model(
        id='gpt-5-pro',
        name='GPT-5 Pro',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=120.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=272000,
    ),
    "openai/gpt-5.1": Model(
        id='gpt-5.1',
        name='GPT-5.1',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.1-chat-latest": Model(
        id='gpt-5.1-chat-latest',
        name='GPT-5.1 Chat',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-5.1-codex": Model(
        id='gpt-5.1-codex',
        name='GPT-5.1 Codex',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.1-codex-max": Model(
        id='gpt-5.1-codex-max',
        name='GPT-5.1 Codex Max',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.1-codex-mini": Model(
        id='gpt-5.1-codex-mini',
        name='GPT-5.1 Codex mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.2": Model(
        id='gpt-5.2',
        name='GPT-5.2',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.2-chat-latest": Model(
        id='gpt-5.2-chat-latest',
        name='GPT-5.2 Chat',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openai/gpt-5.2-codex": Model(
        id='gpt-5.2-codex',
        name='GPT-5.2 Codex',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.2-pro": Model(
        id='gpt-5.2-pro',
        name='GPT-5.2 Pro',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=21.0, output=168.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.3-codex": Model(
        id='gpt-5.3-codex',
        name='GPT-5.3 Codex',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openai/gpt-5.3-codex-spark": Model(
        id='gpt-5.3-codex-spark',
        name='GPT-5.3 Codex Spark',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
    ),
    "openai/o1": Model(
        id='o1',
        name='o1',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=60.0, cache_read=7.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o1-pro": Model(
        id='o1-pro',
        name='o1-pro',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=150.0, output=600.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o3": Model(
        id='o3',
        name='o3',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o3-deep-research": Model(
        id='o3-deep-research',
        name='o3-deep-research',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=40.0, cache_read=2.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o3-mini": Model(
        id='o3-mini',
        name='o3-mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o3-pro": Model(
        id='o3-pro',
        name='o3-pro',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=20.0, output=80.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o4-mini": Model(
        id='o4-mini',
        name='o4-mini',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.28, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openai/o4-mini-deep-research": Model(
        id='o4-mini-deep-research',
        name='o4-mini-deep-research',
        api='openai-responses',
        provider='openai',
        base_url='https://api.openai.com/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "opencode/big-pickle": Model(
        id='big-pickle',
        name='Big Pickle',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=128000,
    ),
    "opencode/claude-3-5-haiku": Model(
        id='claude-3-5-haiku',
        name='Claude Haiku 3.5',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.8, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "opencode/claude-haiku-4-5": Model(
        id='claude-haiku-4-5',
        name='Claude Haiku 4.5',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.1, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "opencode/claude-opus-4-1": Model(
        id='claude-opus-4-1',
        name='Claude Opus 4.1',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "opencode/claude-opus-4-5": Model(
        id='claude-opus-4-5',
        name='Claude Opus 4.5',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "opencode/claude-opus-4-6": Model(
        id='claude-opus-4-6',
        name='Claude Opus 4.6',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=128000,
    ),
    "opencode/claude-sonnet-4": Model(
        id='claude-sonnet-4',
        name='Claude Sonnet 4',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "opencode/claude-sonnet-4-5": Model(
        id='claude-sonnet-4-5',
        name='Claude Sonnet 4.5',
        api='anthropic-messages',
        provider='opencode',
        base_url='https://opencode.ai/zen',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "opencode/gemini-3-flash": Model(
        id='gemini-3-flash',
        name='Gemini 3 Flash',
        api='google-generative-ai',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.05, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "opencode/gemini-3-pro": Model(
        id='gemini-3-pro',
        name='Gemini 3 Pro',
        api='google-generative-ai',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.2, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "opencode/glm-4.6": Model(
        id='glm-4.6',
        name='GLM-4.6',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.1, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/glm-4.7": Model(
        id='glm-4.7',
        name='GLM-4.7',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.1, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/glm-5": Model(
        id='glm-5',
        name='GLM-5',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.2, cache_read=0.2, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/glm-5-free": Model(
        id='glm-5-free',
        name='GLM-5 Free',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/gpt-5": Model(
        id='gpt-5',
        name='GPT-5',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.07, output=8.5, cache_read=0.107, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5-codex": Model(
        id='gpt-5-codex',
        name='GPT-5 Codex',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.07, output=8.5, cache_read=0.107, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5-nano": Model(
        id='gpt-5-nano',
        name='GPT-5 Nano',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.1": Model(
        id='gpt-5.1',
        name='GPT-5.1',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.07, output=8.5, cache_read=0.107, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.1-codex": Model(
        id='gpt-5.1-codex',
        name='GPT-5.1 Codex',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.07, output=8.5, cache_read=0.107, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.1-codex-max": Model(
        id='gpt-5.1-codex-max',
        name='GPT-5.1 Codex Max',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.1-codex-mini": Model(
        id='gpt-5.1-codex-mini',
        name='GPT-5.1 Codex Mini',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.025, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.2": Model(
        id='gpt-5.2',
        name='GPT-5.2',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/gpt-5.2-codex": Model(
        id='gpt-5.2-codex',
        name='GPT-5.2 Codex',
        api='openai-responses',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "opencode/kimi-k2": Model(
        id='kimi-k2',
        name='Kimi K2',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.4, output=2.5, cache_read=0.4, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "opencode/kimi-k2-thinking": Model(
        id='kimi-k2-thinking',
        name='Kimi K2 Thinking',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.4, output=2.5, cache_read=0.4, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "opencode/kimi-k2.5": Model(
        id='kimi-k2.5',
        name='Kimi K2.5',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=3.0, cache_read=0.08, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "opencode/kimi-k2.5-free": Model(
        id='kimi-k2.5-free',
        name='Kimi K2.5 Free',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "opencode/minimax-m2.1": Model(
        id='minimax-m2.1',
        name='MiniMax M2.1',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.1, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/minimax-m2.5": Model(
        id='minimax-m2.5',
        name='MiniMax M2.5',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.06, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "opencode/minimax-m2.5-free": Model(
        id='minimax-m2.5-free',
        name='MiniMax M2.5 Free',
        api='openai-completions',
        provider='opencode',
        base_url='https://opencode.ai/zen/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "openrouter/ai21/jamba-large-1.7": Model(
        id='ai21/jamba-large-1.7',
        name='AI21: Jamba Large 1.7',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=4096,
    ),
    "openrouter/alibaba/tongyi-deepresearch-30b-a3b": Model(
        id='alibaba/tongyi-deepresearch-30b-a3b',
        name='Tongyi DeepResearch 30B A3B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09, output=0.44999999999999996, cache_read=0.09, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/allenai/olmo-3.1-32b-instruct": Model(
        id='allenai/olmo-3.1-32b-instruct',
        name='AllenAI: Olmo 3.1 32B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=65536,
        max_tokens=4096,
    ),
    "openrouter/amazon/nova-2-lite-v1": Model(
        id='amazon/nova-2-lite-v1',
        name='Amazon: Nova 2 Lite',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=65535,
    ),
    "openrouter/amazon/nova-lite-v1": Model(
        id='amazon/nova-lite-v1',
        name='Amazon: Nova Lite 1.0',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.06, output=0.24, cache_read=0.0, cache_write=0.0),
        context_window=300000,
        max_tokens=5120,
    ),
    "openrouter/amazon/nova-micro-v1": Model(
        id='amazon/nova-micro-v1',
        name='Amazon: Nova Micro 1.0',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.035, output=0.14, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=5120,
    ),
    "openrouter/amazon/nova-premier-v1": Model(
        id='amazon/nova-premier-v1',
        name='Amazon: Nova Premier 1.0',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=12.5, cache_read=0.625, cache_write=0.0),
        context_window=1000000,
        max_tokens=32000,
    ),
    "openrouter/amazon/nova-pro-v1": Model(
        id='amazon/nova-pro-v1',
        name='Amazon: Nova Pro 1.0',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.7999999999999999, output=3.1999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=300000,
        max_tokens=5120,
    ),
    "openrouter/anthropic/claude-3-haiku": Model(
        id='anthropic/claude-3-haiku',
        name='Anthropic: Claude 3 Haiku',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=1.25, cache_read=0.03, cache_write=0.3),
        context_window=200000,
        max_tokens=4096,
    ),
    "openrouter/anthropic/claude-3.5-haiku": Model(
        id='anthropic/claude-3.5-haiku',
        name='Anthropic: Claude 3.5 Haiku',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.7999999999999999, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "openrouter/anthropic/claude-3.5-sonnet": Model(
        id='anthropic/claude-3.5-sonnet',
        name='Anthropic: Claude 3.5 Sonnet',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=6.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "openrouter/anthropic/claude-3.7-sonnet": Model(
        id='anthropic/claude-3.7-sonnet',
        name='Anthropic: Claude 3.7 Sonnet',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-3.7-sonnet:thinking": Model(
        id='anthropic/claude-3.7-sonnet:thinking',
        name='Anthropic: Claude 3.7 Sonnet (thinking)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-haiku-4.5": Model(
        id='anthropic/claude-haiku-4.5',
        name='Anthropic: Claude Haiku 4.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.09999999999999999, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-opus-4": Model(
        id='anthropic/claude-opus-4',
        name='Anthropic: Claude Opus 4',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "openrouter/anthropic/claude-opus-4.1": Model(
        id='anthropic/claude-opus-4.1',
        name='Anthropic: Claude Opus 4.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "openrouter/anthropic/claude-opus-4.5": Model(
        id='anthropic/claude-opus-4.5',
        name='Anthropic: Claude Opus 4.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-opus-4.6": Model(
        id='anthropic/claude-opus-4.6',
        name='Anthropic: Claude Opus 4.6',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=1000000,
        max_tokens=128000,
    ),
    "openrouter/anthropic/claude-sonnet-4": Model(
        id='anthropic/claude-sonnet-4',
        name='Anthropic: Claude Sonnet 4',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-sonnet-4.5": Model(
        id='anthropic/claude-sonnet-4.5',
        name='Anthropic: Claude Sonnet 4.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=64000,
    ),
    "openrouter/anthropic/claude-sonnet-4.6": Model(
        id='anthropic/claude-sonnet-4.6',
        name='Anthropic: Claude Sonnet 4.6',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=128000,
    ),
    "openrouter/arcee-ai/trinity-large-preview:free": Model(
        id='arcee-ai/trinity-large-preview:free',
        name='Arcee AI: Trinity Large Preview (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=4096,
    ),
    "openrouter/arcee-ai/trinity-mini": Model(
        id='arcee-ai/trinity-mini',
        name='Arcee AI: Trinity Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.045, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/arcee-ai/trinity-mini:free": Model(
        id='arcee-ai/trinity-mini:free',
        name='Arcee AI: Trinity Mini (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/arcee-ai/virtuoso-large": Model(
        id='arcee-ai/virtuoso-large',
        name='Arcee AI: Virtuoso Large',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.75, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=64000,
    ),
    "openrouter/auto": Model(
        id='auto',
        name='Auto',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "openrouter/baidu/ernie-4.5-21b-a3b": Model(
        id='baidu/ernie-4.5-21b-a3b',
        name='Baidu: ERNIE 4.5 21B A3B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.28, cache_read=0.0, cache_write=0.0),
        context_window=120000,
        max_tokens=8000,
    ),
    "openrouter/baidu/ernie-4.5-vl-28b-a3b": Model(
        id='baidu/ernie-4.5-vl-28b-a3b',
        name='Baidu: ERNIE 4.5 VL 28B A3B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.14, output=0.56, cache_read=0.0, cache_write=0.0),
        context_window=30000,
        max_tokens=8000,
    ),
    "openrouter/bytedance-seed/seed-1.6": Model(
        id='bytedance-seed/seed-1.6',
        name='ByteDance Seed: Seed 1.6',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
    ),
    "openrouter/bytedance-seed/seed-1.6-flash": Model(
        id='bytedance-seed/seed-1.6-flash',
        name='ByteDance Seed: Seed 1.6 Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
    ),
    "openrouter/cohere/command-r-08-2024": Model(
        id='cohere/command-r-08-2024',
        name='Cohere: Command R (08-2024)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "openrouter/cohere/command-r-plus-08-2024": Model(
        id='cohere/command-r-plus-08-2024',
        name='Cohere: Command R+ (08-2024)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "openrouter/deepseek/deepseek-chat": Model(
        id='deepseek/deepseek-chat',
        name='DeepSeek: DeepSeek V3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.15, cache_write=0.0),
        context_window=163840,
        max_tokens=163840,
    ),
    "openrouter/deepseek/deepseek-chat-v3-0324": Model(
        id='deepseek/deepseek-chat-v3-0324',
        name='DeepSeek: DeepSeek V3 0324',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19, output=0.87, cache_read=0.095, cache_write=0.0),
        context_window=163840,
        max_tokens=65536,
    ),
    "openrouter/deepseek/deepseek-chat-v3.1": Model(
        id='deepseek/deepseek-chat-v3.1',
        name='DeepSeek: DeepSeek V3.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.75, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=7168,
    ),
    "openrouter/deepseek/deepseek-r1": Model(
        id='deepseek/deepseek-r1',
        name='DeepSeek: R1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.7, output=2.5, cache_read=0.0, cache_write=0.0),
        context_window=64000,
        max_tokens=16000,
    ),
    "openrouter/deepseek/deepseek-r1-0528": Model(
        id='deepseek/deepseek-r1-0528',
        name='DeepSeek: R1 0528',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.75, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=163840,
        max_tokens=65536,
    ),
    "openrouter/deepseek/deepseek-v3.1-terminus": Model(
        id='deepseek/deepseek-v3.1-terminus',
        name='DeepSeek: DeepSeek V3.1 Terminus',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.21, output=0.7899999999999999, cache_read=0.1300000002, cache_write=0.0),
        context_window=163840,
        max_tokens=4096,
    ),
    "openrouter/deepseek/deepseek-v3.1-terminus:exacto": Model(
        id='deepseek/deepseek-v3.1-terminus:exacto',
        name='DeepSeek: DeepSeek V3.1 Terminus (exacto)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.21, output=0.7899999999999999, cache_read=0.16799999999999998, cache_write=0.0),
        context_window=163840,
        max_tokens=4096,
    ),
    "openrouter/deepseek/deepseek-v3.2": Model(
        id='deepseek/deepseek-v3.2',
        name='DeepSeek: DeepSeek V3.2',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.26, output=0.38, cache_read=0.13, cache_write=0.0),
        context_window=163840,
        max_tokens=4096,
    ),
    "openrouter/deepseek/deepseek-v3.2-exp": Model(
        id='deepseek/deepseek-v3.2-exp',
        name='DeepSeek: DeepSeek V3.2 Exp',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.27, output=0.41, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=65536,
    ),
    "openrouter/google/gemini-2.0-flash-001": Model(
        id='google/gemini-2.0-flash-001',
        name='Google: Gemini 2.0 Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.024999999999999998, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=8192,
    ),
    "openrouter/google/gemini-2.0-flash-lite-001": Model(
        id='google/gemini-2.0-flash-lite-001',
        name='Google: Gemini 2.0 Flash Lite',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=8192,
    ),
    "openrouter/google/gemini-2.5-flash": Model(
        id='google/gemini-2.5-flash',
        name='Google: Gemini 2.5 Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.03, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=65535,
    ),
    "openrouter/google/gemini-2.5-flash-lite": Model(
        id='google/gemini-2.5-flash-lite',
        name='Google: Gemini 2.5 Flash Lite',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.01, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=65535,
    ),
    "openrouter/google/gemini-2.5-flash-lite-preview-09-2025": Model(
        id='google/gemini-2.5-flash-lite-preview-09-2025',
        name='Google: Gemini 2.5 Flash Lite Preview 09-2025',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.01, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=65535,
    ),
    "openrouter/google/gemini-2.5-flash-preview-09-2025": Model(
        id='google/gemini-2.5-flash-preview-09-2025',
        name='Google: Gemini 2.5 Flash Preview 09-2025',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.03, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=65536,
    ),
    "openrouter/google/gemini-2.5-pro": Model(
        id='google/gemini-2.5-pro',
        name='Google: Gemini 2.5 Pro',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.375),
        context_window=1048576,
        max_tokens=65536,
    ),
    "openrouter/google/gemini-2.5-pro-preview": Model(
        id='google/gemini-2.5-pro-preview',
        name='Google: Gemini 2.5 Pro Preview 06-05',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.375),
        context_window=1048576,
        max_tokens=65536,
    ),
    "openrouter/google/gemini-2.5-pro-preview-05-06": Model(
        id='google/gemini-2.5-pro-preview-05-06',
        name='Google: Gemini 2.5 Pro Preview 05-06',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.375),
        context_window=1048576,
        max_tokens=65535,
    ),
    "openrouter/google/gemini-3-flash-preview": Model(
        id='google/gemini-3-flash-preview',
        name='Google: Gemini 3 Flash Preview',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.049999999999999996, cache_write=0.08333333333333334),
        context_window=1048576,
        max_tokens=65535,
    ),
    "openrouter/google/gemini-3-pro-preview": Model(
        id='google/gemini-3-pro-preview',
        name='Google: Gemini 3 Pro Preview',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.19999999999999998, cache_write=0.375),
        context_window=1048576,
        max_tokens=65536,
    ),
    "openrouter/google/gemma-3-27b-it": Model(
        id='google/gemma-3-27b-it',
        name='Google: Gemma 3 27B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.04, output=0.15, cache_read=0.02, cache_write=0.0),
        context_window=128000,
        max_tokens=65536,
    ),
    "openrouter/google/gemma-3-27b-it:free": Model(
        id='google/gemma-3-27b-it:free',
        name='Google: Gemma 3 27B (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "openrouter/inception/mercury": Model(
        id='inception/mercury',
        name='Inception: Mercury',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/inception/mercury-coder": Model(
        id='inception/mercury-coder',
        name='Inception: Mercury Coder',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/kwaipilot/kat-coder-pro": Model(
        id='kwaipilot/kat-coder-pro',
        name='Kwaipilot: KAT-Coder-Pro V1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.207, output=0.828, cache_read=0.0414, cache_write=0.0),
        context_window=256000,
        max_tokens=128000,
    ),
    "openrouter/meta-llama/llama-3-8b-instruct": Model(
        id='meta-llama/llama-3-8b-instruct',
        name='Meta: Llama 3 8B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.03, output=0.04, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=16384,
    ),
    "openrouter/meta-llama/llama-3.1-405b-instruct": Model(
        id='meta-llama/llama-3.1-405b-instruct',
        name='Meta: Llama 3.1 405B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=4.0, output=4.0, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=4096,
    ),
    "openrouter/meta-llama/llama-3.1-70b-instruct": Model(
        id='meta-llama/llama-3.1-70b-instruct',
        name='Meta: Llama 3.1 70B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=0.39999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/meta-llama/llama-3.1-8b-instruct": Model(
        id='meta-llama/llama-3.1-8b-instruct',
        name='Meta: Llama 3.1 8B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.02, output=0.049999999999999996, cache_read=0.0, cache_write=0.0),
        context_window=16384,
        max_tokens=16384,
    ),
    "openrouter/meta-llama/llama-3.3-70b-instruct": Model(
        id='meta-llama/llama-3.3-70b-instruct',
        name='Meta: Llama 3.3 70B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.32, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "openrouter/meta-llama/llama-3.3-70b-instruct:free": Model(
        id='meta-llama/llama-3.3-70b-instruct:free',
        name='Meta: Llama 3.3 70B Instruct (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "openrouter/meta-llama/llama-4-maverick": Model(
        id='meta-llama/llama-4-maverick',
        name='Meta: Llama 4 Maverick',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=16384,
    ),
    "openrouter/meta-llama/llama-4-scout": Model(
        id='meta-llama/llama-4-scout',
        name='Meta: Llama 4 Scout',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.08, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=327680,
        max_tokens=16384,
    ),
    "openrouter/minimax/minimax-m1": Model(
        id='minimax/minimax-m1',
        name='MiniMax: MiniMax M1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=2.2, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=40000,
    ),
    "openrouter/minimax/minimax-m2": Model(
        id='minimax/minimax-m2',
        name='MiniMax: MiniMax M2',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.255, output=1.0, cache_read=0.03, cache_write=0.0),
        context_window=196608,
        max_tokens=65536,
    ),
    "openrouter/minimax/minimax-m2.1": Model(
        id='minimax/minimax-m2.1',
        name='MiniMax: MiniMax M2.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.27, output=0.95, cache_read=0.0299999997, cache_write=0.0),
        context_window=196608,
        max_tokens=4096,
    ),
    "openrouter/minimax/minimax-m2.5": Model(
        id='minimax/minimax-m2.5',
        name='MiniMax: MiniMax M2.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.1, cache_read=0.15, cache_write=0.0),
        context_window=196608,
        max_tokens=65536,
    ),
    "openrouter/mistralai/codestral-2508": Model(
        id='mistralai/codestral-2508',
        name='Mistral: Codestral 2508',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.8999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=4096,
    ),
    "openrouter/mistralai/devstral-2512": Model(
        id='mistralai/devstral-2512',
        name='Mistral: Devstral 2 2512',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.22, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/mistralai/devstral-medium": Model(
        id='mistralai/devstral-medium',
        name='Mistral: Devstral Medium',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/devstral-small": Model(
        id='mistralai/devstral-small',
        name='Mistral: Devstral Small 1.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/ministral-14b-2512": Model(
        id='mistralai/ministral-14b-2512',
        name='Mistral: Ministral 3 14B 2512',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.19999999999999998, output=0.19999999999999998, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/mistralai/ministral-3b-2512": Model(
        id='mistralai/ministral-3b-2512',
        name='Mistral: Ministral 3 3B 2512',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.09999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/ministral-8b-2512": Model(
        id='mistralai/ministral-8b-2512',
        name='Mistral: Ministral 3 8B 2512',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-large": Model(
        id='mistralai/mistral-large',
        name='Mistral Large',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-large-2407": Model(
        id='mistralai/mistral-large-2407',
        name='Mistral Large 2407',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-large-2411": Model(
        id='mistralai/mistral-large-2411',
        name='Mistral Large 2411',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-large-2512": Model(
        id='mistralai/mistral-large-2512',
        name='Mistral: Mistral Large 3 2512',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-medium-3": Model(
        id='mistralai/mistral-medium-3',
        name='Mistral: Mistral Medium 3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-medium-3.1": Model(
        id='mistralai/mistral-medium-3.1',
        name='Mistral: Mistral Medium 3.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-nemo": Model(
        id='mistralai/mistral-nemo',
        name='Mistral: Mistral Nemo',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.02, output=0.04, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "openrouter/mistralai/mistral-saba": Model(
        id='mistralai/mistral-saba',
        name='Mistral: Saba',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-small-24b-instruct-2501": Model(
        id='mistralai/mistral-small-24b-instruct-2501',
        name='Mistral: Mistral Small 3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.08, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=16384,
    ),
    "openrouter/mistralai/mistral-small-3.1-24b-instruct": Model(
        id='mistralai/mistral-small-3.1-24b-instruct',
        name='Mistral: Mistral Small 3.1 24B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.03, output=0.11, cache_read=0.015, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/mistralai/mistral-small-3.1-24b-instruct:free": Model(
        id='mistralai/mistral-small-3.1-24b-instruct:free',
        name='Mistral: Mistral Small 3.1 24B (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mistral-small-3.2-24b-instruct": Model(
        id='mistralai/mistral-small-3.2-24b-instruct',
        name='Mistral: Mistral Small 3.2 24B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.06, output=0.18, cache_read=0.03, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/mistralai/mistral-small-creative": Model(
        id='mistralai/mistral-small-creative',
        name='Mistral: Mistral Small Creative',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mixtral-8x22b-instruct": Model(
        id='mistralai/mixtral-8x22b-instruct',
        name='Mistral: Mixtral 8x22B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=65536,
        max_tokens=4096,
    ),
    "openrouter/mistralai/mixtral-8x7b-instruct": Model(
        id='mistralai/mixtral-8x7b-instruct',
        name='Mistral: Mixtral 8x7B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.54, output=0.54, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=16384,
    ),
    "openrouter/mistralai/pixtral-large-2411": Model(
        id='mistralai/pixtral-large-2411',
        name='Mistral: Pixtral Large 2411',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/mistralai/voxtral-small-24b-2507": Model(
        id='mistralai/voxtral-small-24b-2507',
        name='Mistral: Voxtral Small 24B 2507',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=4096,
    ),
    "openrouter/moonshotai/kimi-k2": Model(
        id='moonshotai/kimi-k2',
        name='MoonshotAI: Kimi K2 0711',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.5, output=2.4, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/moonshotai/kimi-k2-0905": Model(
        id='moonshotai/kimi-k2-0905',
        name='MoonshotAI: Kimi K2 0905',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=2.0, cache_read=0.15, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/moonshotai/kimi-k2-0905:exacto": Model(
        id='moonshotai/kimi-k2-0905:exacto',
        name='MoonshotAI: Kimi K2 0905 (exacto)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.5, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/moonshotai/kimi-k2-thinking": Model(
        id='moonshotai/kimi-k2-thinking',
        name='MoonshotAI: Kimi K2 Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.75, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=262144,
        max_tokens=65535,
    ),
    "openrouter/moonshotai/kimi-k2.5": Model(
        id='moonshotai/kimi-k2.5',
        name='MoonshotAI: Kimi K2.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.22999999999999998, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "openrouter/nex-agi/deepseek-v3.1-nex-n1": Model(
        id='nex-agi/deepseek-v3.1-nex-n1',
        name='Nex AGI: DeepSeek V3.1 Nex N1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.27, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=163840,
    ),
    "openrouter/nousresearch/deephermes-3-mistral-24b-preview": Model(
        id='nousresearch/deephermes-3-mistral-24b-preview',
        name='Nous: DeepHermes 3 Mistral 24B Preview',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.02, output=0.09999999999999999, cache_read=0.01, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "openrouter/nousresearch/hermes-4-70b": Model(
        id='nousresearch/hermes-4-70b',
        name='Nous: Hermes 4 70B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.11, output=0.38, cache_read=0.055, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/nvidia/llama-3.1-nemotron-70b-instruct": Model(
        id='nvidia/llama-3.1-nemotron-70b-instruct',
        name='NVIDIA: Llama 3.1 Nemotron 70B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.2, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "openrouter/nvidia/llama-3.3-nemotron-super-49b-v1.5": Model(
        id='nvidia/llama-3.3-nemotron-super-49b-v1.5',
        name='NVIDIA: Llama 3.3 Nemotron Super 49B V1.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/nvidia/nemotron-3-nano-30b-a3b": Model(
        id='nvidia/nemotron-3-nano-30b-a3b',
        name='NVIDIA: Nemotron 3 Nano 30B A3B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.19999999999999998, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/nvidia/nemotron-3-nano-30b-a3b:free": Model(
        id='nvidia/nemotron-3-nano-30b-a3b:free',
        name='NVIDIA: Nemotron 3 Nano 30B A3B (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=4096,
    ),
    "openrouter/nvidia/nemotron-nano-12b-v2-vl:free": Model(
        id='nvidia/nemotron-nano-12b-v2-vl:free',
        name='NVIDIA: Nemotron Nano 12B 2 VL (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=128000,
    ),
    "openrouter/nvidia/nemotron-nano-9b-v2": Model(
        id='nvidia/nemotron-nano-9b-v2',
        name='NVIDIA: Nemotron Nano 9B V2',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.16, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/nvidia/nemotron-nano-9b-v2:free": Model(
        id='nvidia/nemotron-nano-9b-v2:free',
        name='NVIDIA: Nemotron Nano 9B V2 (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-3.5-turbo": Model(
        id='openai/gpt-3.5-turbo',
        name='OpenAI: GPT-3.5 Turbo',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.5, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=16385,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-3.5-turbo-0613": Model(
        id='openai/gpt-3.5-turbo-0613',
        name='OpenAI: GPT-3.5 Turbo (older v0613)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=4095,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-3.5-turbo-16k": Model(
        id='openai/gpt-3.5-turbo-16k',
        name='OpenAI: GPT-3.5 Turbo 16k',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=4.0, cache_read=0.0, cache_write=0.0),
        context_window=16385,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4": Model(
        id='openai/gpt-4',
        name='OpenAI: GPT-4',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=30.0, output=60.0, cache_read=0.0, cache_write=0.0),
        context_window=8191,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4-0314": Model(
        id='openai/gpt-4-0314',
        name='OpenAI: GPT-4 (older v0314)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=30.0, output=60.0, cache_read=0.0, cache_write=0.0),
        context_window=8191,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4-1106-preview": Model(
        id='openai/gpt-4-1106-preview',
        name='OpenAI: GPT-4 Turbo (older v1106)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4-turbo": Model(
        id='openai/gpt-4-turbo',
        name='OpenAI: GPT-4 Turbo',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4-turbo-preview": Model(
        id='openai/gpt-4-turbo-preview',
        name='OpenAI: GPT-4 Turbo Preview',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4.1": Model(
        id='openai/gpt-4.1',
        name='OpenAI: GPT-4.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openrouter/openai/gpt-4.1-mini": Model(
        id='openai/gpt-4.1-mini',
        name='OpenAI: GPT-4.1 Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=1.5999999999999999, cache_read=0.09999999999999999, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openrouter/openai/gpt-4.1-nano": Model(
        id='openai/gpt-4.1-nano',
        name='OpenAI: GPT-4.1 Nano',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "openrouter/openai/gpt-4o": Model(
        id='openai/gpt-4o',
        name='OpenAI: GPT-4o',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o-2024-05-13": Model(
        id='openai/gpt-4o-2024-05-13',
        name='OpenAI: GPT-4o (2024-05-13)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-4o-2024-08-06": Model(
        id='openai/gpt-4o-2024-08-06',
        name='OpenAI: GPT-4o (2024-08-06)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o-2024-11-20": Model(
        id='openai/gpt-4o-2024-11-20',
        name='OpenAI: GPT-4o (2024-11-20)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o-audio-preview": Model(
        id='openai/gpt-4o-audio-preview',
        name='OpenAI: GPT-4o Audio',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o-mini": Model(
        id='openai/gpt-4o-mini',
        name='OpenAI: GPT-4o-mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.075, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o-mini-2024-07-18": Model(
        id='openai/gpt-4o-mini-2024-07-18',
        name='OpenAI: GPT-4o-mini (2024-07-18)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.075, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-4o:extended": Model(
        id='openai/gpt-4o:extended',
        name='OpenAI: GPT-4o (extended)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=6.0, output=18.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
    ),
    "openrouter/openai/gpt-5": Model(
        id='openai/gpt-5',
        name='OpenAI: GPT-5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-codex": Model(
        id='openai/gpt-5-codex',
        name='OpenAI: GPT-5 Codex',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-image": Model(
        id='openai/gpt-5-image',
        name='OpenAI: GPT-5 Image',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-image-mini": Model(
        id='openai/gpt-5-image-mini',
        name='OpenAI: GPT-5 Image Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=2.0, cache_read=0.25, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-mini": Model(
        id='openai/gpt-5-mini',
        name='OpenAI: GPT-5 Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-nano": Model(
        id='openai/gpt-5-nano',
        name='OpenAI: GPT-5 Nano',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.049999999999999996, output=0.39999999999999997, cache_read=0.005, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5-pro": Model(
        id='openai/gpt-5-pro',
        name='OpenAI: GPT-5 Pro',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=120.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.1": Model(
        id='openai/gpt-5.1',
        name='OpenAI: GPT-5.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.1-chat": Model(
        id='openai/gpt-5.1-chat',
        name='OpenAI: GPT-5.1 Chat',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-5.1-codex": Model(
        id='openai/gpt-5.1-codex',
        name='OpenAI: GPT-5.1-Codex',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.1-codex-max": Model(
        id='openai/gpt-5.1-codex-max',
        name='OpenAI: GPT-5.1-Codex-Max',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.1-codex-mini": Model(
        id='openai/gpt-5.1-codex-mini',
        name='OpenAI: GPT-5.1-Codex-Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=400000,
        max_tokens=100000,
    ),
    "openrouter/openai/gpt-5.2": Model(
        id='openai/gpt-5.2',
        name='OpenAI: GPT-5.2',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.2-chat": Model(
        id='openai/gpt-5.2-chat',
        name='OpenAI: GPT-5.2 Chat',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "openrouter/openai/gpt-5.2-codex": Model(
        id='openai/gpt-5.2-codex',
        name='OpenAI: GPT-5.2-Codex',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-5.2-pro": Model(
        id='openai/gpt-5.2-pro',
        name='OpenAI: GPT-5.2 Pro',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=21.0, output=168.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "openrouter/openai/gpt-oss-120b": Model(
        id='openai/gpt-oss-120b',
        name='OpenAI: gpt-oss-120b',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.039, output=0.19, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-oss-120b:exacto": Model(
        id='openai/gpt-oss-120b:exacto',
        name='OpenAI: gpt-oss-120b (exacto)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.039, output=0.19, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-oss-120b:free": Model(
        id='openai/gpt-oss-120b:free',
        name='OpenAI: gpt-oss-120b (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/openai/gpt-oss-20b": Model(
        id='openai/gpt-oss-20b',
        name='OpenAI: gpt-oss-20b',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.03, output=0.14, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/openai/gpt-oss-20b:free": Model(
        id='openai/gpt-oss-20b:free',
        name='OpenAI: gpt-oss-20b (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/openai/gpt-oss-safeguard-20b": Model(
        id='openai/gpt-oss-safeguard-20b',
        name='OpenAI: gpt-oss-safeguard-20b',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.037, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "openrouter/openai/o1": Model(
        id='openai/o1',
        name='OpenAI: o1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=60.0, cache_read=7.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o3": Model(
        id='openai/o3',
        name='OpenAI: o3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o3-deep-research": Model(
        id='openai/o3-deep-research',
        name='OpenAI: o3 Deep Research',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=40.0, cache_read=2.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o3-mini": Model(
        id='openai/o3-mini',
        name='OpenAI: o3 Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o3-mini-high": Model(
        id='openai/o3-mini-high',
        name='OpenAI: o3 Mini High',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o3-pro": Model(
        id='openai/o3-pro',
        name='OpenAI: o3 Pro',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=20.0, output=80.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o4-mini": Model(
        id='openai/o4-mini',
        name='OpenAI: o4 Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.275, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o4-mini-deep-research": Model(
        id='openai/o4-mini-deep-research',
        name='OpenAI: o4 Mini Deep Research',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openai/o4-mini-high": Model(
        id='openai/o4-mini-high',
        name='OpenAI: o4 Mini High',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.275, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "openrouter/openrouter/aurora-alpha": Model(
        id='openrouter/aurora-alpha',
        name='Aurora Alpha',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=50000,
    ),
    "openrouter/openrouter/auto": Model(
        id='openrouter/auto',
        name='Auto Router',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=2000000,
        max_tokens=4096,
    ),
    "openrouter/openrouter/free": Model(
        id='openrouter/free',
        name='Free Models Router',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=4096,
    ),
    "openrouter/prime-intellect/intellect-3": Model(
        id='prime-intellect/intellect-3',
        name='Prime Intellect: INTELLECT-3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=1.1, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/qwen/qwen-2.5-72b-instruct": Model(
        id='qwen/qwen-2.5-72b-instruct',
        name='Qwen2.5 72B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.12, output=0.39, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=16384,
    ),
    "openrouter/qwen/qwen-2.5-7b-instruct": Model(
        id='qwen/qwen-2.5-7b-instruct',
        name='Qwen: Qwen2.5 7B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.09999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen-max": Model(
        id='qwen/qwen-max',
        name='Qwen: Qwen-Max ',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.5999999999999999, output=6.3999999999999995, cache_read=0.32, cache_write=0.0),
        context_window=32768,
        max_tokens=8192,
    ),
    "openrouter/qwen/qwen-plus": Model(
        id='qwen/qwen-plus',
        name='Qwen: Qwen-Plus',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.2, cache_read=0.08, cache_write=0.0),
        context_window=1000000,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen-plus-2025-07-28": Model(
        id='qwen/qwen-plus-2025-07-28',
        name='Qwen: Qwen Plus 0728',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen-plus-2025-07-28:thinking": Model(
        id='qwen/qwen-plus-2025-07-28:thinking',
        name='Qwen: Qwen Plus 0728 (thinking)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen-turbo": Model(
        id='qwen/qwen-turbo',
        name='Qwen: Qwen-Turbo',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.19999999999999998, cache_read=0.01, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "openrouter/qwen/qwen-vl-max": Model(
        id='qwen/qwen-vl-max',
        name='Qwen: Qwen VL Max',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.7999999999999999, output=3.1999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-14b": Model(
        id='qwen/qwen3-14b',
        name='Qwen: Qwen3 14B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.22, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=40960,
        max_tokens=40960,
    ),
    "openrouter/qwen/qwen3-235b-a22b": Model(
        id='qwen/qwen3-235b-a22b',
        name='Qwen: Qwen3 235B A22B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.15, cache_write=0.0),
        context_window=40960,
        max_tokens=40960,
    ),
    "openrouter/qwen/qwen3-235b-a22b-2507": Model(
        id='qwen/qwen3-235b-a22b-2507',
        name='Qwen: Qwen3 235B A22B Instruct 2507',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.071, output=0.09999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-235b-a22b-thinking-2507": Model(
        id='qwen/qwen3-235b-a22b-thinking-2507',
        name='Qwen: Qwen3 235B A22B Thinking 2507',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-30b-a3b": Model(
        id='qwen/qwen3-30b-a3b',
        name='Qwen: Qwen3 30B A3B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.06, output=0.22, cache_read=0.03, cache_write=0.0),
        context_window=40960,
        max_tokens=40960,
    ),
    "openrouter/qwen/qwen3-30b-a3b-instruct-2507": Model(
        id='qwen/qwen3-30b-a3b-instruct-2507',
        name='Qwen: Qwen3 30B A3B Instruct 2507',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.08, output=0.33, cache_read=0.04, cache_write=0.0),
        context_window=262144,
        max_tokens=262144,
    ),
    "openrouter/qwen/qwen3-30b-a3b-thinking-2507": Model(
        id='qwen/qwen3-30b-a3b-thinking-2507',
        name='Qwen: Qwen3 30B A3B Thinking 2507',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.051, output=0.33999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-32b": Model(
        id='qwen/qwen3-32b',
        name='Qwen: Qwen3 32B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.08, output=0.24, cache_read=0.04, cache_write=0.0),
        context_window=40960,
        max_tokens=40960,
    ),
    "openrouter/qwen/qwen3-4b": Model(
        id='qwen/qwen3-4b',
        name='Qwen: Qwen3 4B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0715, output=0.273, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "openrouter/qwen/qwen3-4b:free": Model(
        id='qwen/qwen3-4b:free',
        name='Qwen: Qwen3 4B (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=40960,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-8b": Model(
        id='qwen/qwen3-8b',
        name='Qwen: Qwen3 8B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.049999999999999996, output=0.39999999999999997, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=32000,
        max_tokens=8192,
    ),
    "openrouter/qwen/qwen3-coder": Model(
        id='qwen/qwen3-coder',
        name='Qwen: Qwen3 Coder 480B A35B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.22, output=1.0, cache_read=0.022, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-coder-30b-a3b-instruct": Model(
        id='qwen/qwen3-coder-30b-a3b-instruct',
        name='Qwen: Qwen3 Coder 30B A3B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.27, cache_read=0.0, cache_write=0.0),
        context_window=160000,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-coder-flash": Model(
        id='qwen/qwen3-coder-flash',
        name='Qwen: Qwen3 Coder Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.5, cache_read=0.06, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-coder-next": Model(
        id='qwen/qwen3-coder-next',
        name='Qwen: Qwen3 Coder Next',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.12, output=0.75, cache_read=0.06, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-coder-plus": Model(
        id='qwen/qwen3-coder-plus',
        name='Qwen: Qwen3 Coder Plus',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-coder:exacto": Model(
        id='qwen/qwen3-coder:exacto',
        name='Qwen: Qwen3 Coder 480B A35B (exacto)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.22, output=1.7999999999999998, cache_read=0.022, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-coder:free": Model(
        id='qwen/qwen3-coder:free',
        name='Qwen: Qwen3 Coder 480B A35B (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=262000,
        max_tokens=262000,
    ),
    "openrouter/qwen/qwen3-max": Model(
        id='qwen/qwen3-max',
        name='Qwen: Qwen3 Max',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.2, output=6.0, cache_read=0.24, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-max-thinking": Model(
        id='qwen/qwen3-max-thinking',
        name='Qwen: Qwen3 Max Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.2, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3-next-80b-a3b-instruct": Model(
        id='qwen/qwen3-next-80b-a3b-instruct',
        name='Qwen: Qwen3 Next 80B A3B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09, output=1.1, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-next-80b-a3b-instruct:free": Model(
        id='qwen/qwen3-next-80b-a3b-instruct:free',
        name='Qwen: Qwen3 Next 80B A3B Instruct (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-next-80b-a3b-thinking": Model(
        id='qwen/qwen3-next-80b-a3b-thinking',
        name='Qwen: Qwen3 Next 80B A3B Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-vl-235b-a22b-instruct": Model(
        id='qwen/qwen3-vl-235b-a22b-instruct',
        name='Qwen: Qwen3 VL 235B A22B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.19999999999999998, output=0.88, cache_read=0.11, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/qwen/qwen3-vl-235b-a22b-thinking": Model(
        id='qwen/qwen3-vl-235b-a22b-thinking',
        name='Qwen: Qwen3 VL 235B A22B Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-vl-30b-a3b-instruct": Model(
        id='qwen/qwen3-vl-30b-a3b-instruct',
        name='Qwen: Qwen3 VL 30B A3B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.13, output=0.52, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-vl-30b-a3b-thinking": Model(
        id='qwen/qwen3-vl-30b-a3b-thinking',
        name='Qwen: Qwen3 VL 30B A3B Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-vl-32b-instruct": Model(
        id='qwen/qwen3-vl-32b-instruct',
        name='Qwen: Qwen3 VL 32B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.10400000000000001, output=0.41600000000000004, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-vl-8b-instruct": Model(
        id='qwen/qwen3-vl-8b-instruct',
        name='Qwen: Qwen3 VL 8B Instruct',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.08, output=0.5, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3-vl-8b-thinking": Model(
        id='qwen/qwen3-vl-8b-thinking',
        name='Qwen: Qwen3 VL 8B Thinking',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.117, output=1.365, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=32768,
    ),
    "openrouter/qwen/qwen3.5-397b-a17b": Model(
        id='qwen/qwen3.5-397b-a17b',
        name='Qwen: Qwen3.5 397B A17B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=1.0, cache_read=0.15, cache_write=0.0),
        context_window=262144,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwen3.5-plus-02-15": Model(
        id='qwen/qwen3.5-plus-02-15',
        name='Qwen: Qwen3.5 Plus 2026-02-15',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=2.4, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "openrouter/qwen/qwq-32b": Model(
        id='qwen/qwq-32b',
        name='Qwen: QwQ 32B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=0.39999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "openrouter/relace/relace-search": Model(
        id='relace/relace-search',
        name='Relace: Relace Search',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=128000,
    ),
    "openrouter/sao10k/l3-euryale-70b": Model(
        id='sao10k/l3-euryale-70b',
        name='Sao10k: Llama 3 Euryale 70B v2.1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.48, output=1.48, cache_read=0.0, cache_write=0.0),
        context_window=8192,
        max_tokens=8192,
    ),
    "openrouter/sao10k/l3.1-euryale-70b": Model(
        id='sao10k/l3.1-euryale-70b',
        name='Sao10K: Llama 3.1 Euryale 70B v2.2',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.65, output=0.75, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "openrouter/stepfun/step-3.5-flash": Model(
        id='stepfun/step-3.5-flash',
        name='StepFun: Step 3.5 Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.02, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "openrouter/stepfun/step-3.5-flash:free": Model(
        id='stepfun/step-3.5-flash:free',
        name='StepFun: Step 3.5 Flash (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "openrouter/thedrummer/rocinante-12b": Model(
        id='thedrummer/rocinante-12b',
        name='TheDrummer: Rocinante 12B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.16999999999999998, output=0.43, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "openrouter/thedrummer/unslopnemo-12b": Model(
        id='thedrummer/unslopnemo-12b',
        name='TheDrummer: UnslopNemo 12B',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=0.39999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "openrouter/tngtech/deepseek-r1t2-chimera": Model(
        id='tngtech/deepseek-r1t2-chimera',
        name='TNG: DeepSeek R1T2 Chimera',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.25, output=0.85, cache_read=0.125, cache_write=0.0),
        context_window=163840,
        max_tokens=163840,
    ),
    "openrouter/tngtech/tng-r1t-chimera": Model(
        id='tngtech/tng-r1t-chimera',
        name='TNG: R1T Chimera',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.25, output=0.85, cache_read=0.125, cache_write=0.0),
        context_window=163840,
        max_tokens=65536,
    ),
    "openrouter/upstage/solar-pro-3:free": Model(
        id='upstage/solar-pro-3:free',
        name='Upstage: Solar Pro 3 (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-3": Model(
        id='x-ai/grok-3',
        name='xAI: Grok 3',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-3-beta": Model(
        id='x-ai/grok-3-beta',
        name='xAI: Grok 3 Beta',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-3-mini": Model(
        id='x-ai/grok-3-mini',
        name='xAI: Grok 3 Mini',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-3-mini-beta": Model(
        id='x-ai/grok-3-mini-beta',
        name='xAI: Grok 3 Mini Beta',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-4": Model(
        id='x-ai/grok-4',
        name='xAI: Grok 4',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=256000,
        max_tokens=4096,
    ),
    "openrouter/x-ai/grok-4-fast": Model(
        id='x-ai/grok-4-fast',
        name='xAI: Grok 4 Fast',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "openrouter/x-ai/grok-4.1-fast": Model(
        id='x-ai/grok-4.1-fast',
        name='xAI: Grok 4.1 Fast',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "openrouter/x-ai/grok-code-fast-1": Model(
        id='x-ai/grok-code-fast-1',
        name='xAI: Grok Code Fast 1',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=1.5, cache_read=0.02, cache_write=0.0),
        context_window=256000,
        max_tokens=10000,
    ),
    "openrouter/xiaomi/mimo-v2-flash": Model(
        id='xiaomi/mimo-v2-flash',
        name='Xiaomi: MiMo-V2-Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09, output=0.29, cache_read=0.045, cache_write=0.0),
        context_window=262144,
        max_tokens=4096,
    ),
    "openrouter/z-ai/glm-4-32b": Model(
        id='z-ai/glm-4-32b',
        name='Z.ai: GLM 4 32B ',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.09999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "openrouter/z-ai/glm-4.5": Model(
        id='z-ai/glm-4.5',
        name='Z.ai: GLM 4.5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.35, output=1.55, cache_read=0.175, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "openrouter/z-ai/glm-4.5-air": Model(
        id='z-ai/glm-4.5-air',
        name='Z.ai: GLM 4.5 Air',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.13, output=0.85, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=131072,
        max_tokens=98304,
    ),
    "openrouter/z-ai/glm-4.5-air:free": Model(
        id='z-ai/glm-4.5-air:free',
        name='Z.ai: GLM 4.5 Air (free)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=96000,
    ),
    "openrouter/z-ai/glm-4.5v": Model(
        id='z-ai/glm-4.5v',
        name='Z.ai: GLM 4.5V',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=1.7999999999999998, cache_read=0.11, cache_write=0.0),
        context_window=65536,
        max_tokens=16384,
    ),
    "openrouter/z-ai/glm-4.6": Model(
        id='z-ai/glm-4.6',
        name='Z.ai: GLM 4.6',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.33999999999999997, output=1.7, cache_read=0.16999999999999998, cache_write=0.0),
        context_window=202752,
        max_tokens=65536,
    ),
    "openrouter/z-ai/glm-4.6:exacto": Model(
        id='z-ai/glm-4.6:exacto',
        name='Z.ai: GLM 4.6 (exacto)',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.44, output=1.76, cache_read=0.11, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "openrouter/z-ai/glm-4.6v": Model(
        id='z-ai/glm-4.6v',
        name='Z.ai: GLM 4.6V',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=0.8999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "openrouter/z-ai/glm-4.7": Model(
        id='z-ai/glm-4.7',
        name='Z.ai: GLM 4.7',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.38, output=1.7, cache_read=0.19, cache_write=0.0),
        context_window=202752,
        max_tokens=65535,
    ),
    "openrouter/z-ai/glm-4.7-flash": Model(
        id='z-ai/glm-4.7-flash',
        name='Z.ai: GLM 4.7 Flash',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.06, output=0.39999999999999997, cache_read=0.0100000002, cache_write=0.0),
        context_window=202752,
        max_tokens=4096,
    ),
    "openrouter/z-ai/glm-5": Model(
        id='z-ai/glm-5',
        name='Z.ai: GLM 5',
        api='openai-completions',
        provider='openrouter',
        base_url='https://openrouter.ai/api/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=2.5500000000000003, cache_read=0.0, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/alibaba/qwen-3-14b": Model(
        id='alibaba/qwen-3-14b',
        name='Qwen3-14B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.06, output=0.24, cache_read=0.0, cache_write=0.0),
        context_window=40960,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/alibaba/qwen-3-235b": Model(
        id='alibaba/qwen-3-235b',
        name='Qwen3-235B-A22B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.071, output=0.463, cache_read=0.0, cache_write=0.0),
        context_window=40960,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/alibaba/qwen-3-30b": Model(
        id='alibaba/qwen-3-30b',
        name='Qwen3-30B-A3B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.08, output=0.29, cache_read=0.0, cache_write=0.0),
        context_window=40960,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/alibaba/qwen-3-32b": Model(
        id='alibaba/qwen-3-32b',
        name='Qwen 3 32B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=40960,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/alibaba/qwen3-235b-a22b-thinking": Model(
        id='alibaba/qwen3-235b-a22b-thinking',
        name='Qwen3 235B A22B Thinking 2507',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.9000000000000004, cache_read=0.0, cache_write=0.0),
        context_window=262114,
        max_tokens=262114,
    ),
    "vercel-ai-gateway/alibaba/qwen3-coder": Model(
        id='alibaba/qwen3-coder',
        name='Qwen3 Coder 480B A35B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=1.5999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=66536,
    ),
    "vercel-ai-gateway/alibaba/qwen3-coder-30b-a3b": Model(
        id='alibaba/qwen3-coder-30b-a3b',
        name='Qwen 3 Coder 30B A3B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.27, cache_read=0.0, cache_write=0.0),
        context_window=160000,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/alibaba/qwen3-coder-next": Model(
        id='alibaba/qwen3-coder-next',
        name='Qwen3 Coder Next',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.5, output=1.2, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/alibaba/qwen3-coder-plus": Model(
        id='alibaba/qwen3-coder-plus',
        name='Qwen3 Coder Plus',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/alibaba/qwen3-max-preview": Model(
        id='alibaba/qwen3-max-preview',
        name='Qwen3 Max Preview',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=1.2, output=6.0, cache_read=0.24, cache_write=0.0),
        context_window=262144,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/alibaba/qwen3-max-thinking": Model(
        id='alibaba/qwen3-max-thinking',
        name='Qwen 3 Max Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.2, output=6.0, cache_read=0.24, cache_write=0.0),
        context_window=256000,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/alibaba/qwen3-vl-thinking": Model(
        id='alibaba/qwen3-vl-thinking',
        name='Qwen3 VL 235B A22B Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.22, output=0.88, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/alibaba/qwen3.5-plus": Model(
        id='alibaba/qwen3.5-plus',
        name='Qwen 3.5 Plus',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=2.4, cache_read=0.04, cache_write=0.5),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-3-haiku": Model(
        id='anthropic/claude-3-haiku',
        name='Claude 3 Haiku',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=1.25, cache_read=0.03, cache_write=0.3),
        context_window=200000,
        max_tokens=4096,
    ),
    "vercel-ai-gateway/anthropic/claude-3.5-haiku": Model(
        id='anthropic/claude-3.5-haiku',
        name='Claude 3.5 Haiku',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.7999999999999999, output=4.0, cache_read=0.08, cache_write=1.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/anthropic/claude-3.5-sonnet": Model(
        id='anthropic/claude-3.5-sonnet',
        name='Claude 3.5 Sonnet',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/anthropic/claude-3.5-sonnet-20240620": Model(
        id='anthropic/claude-3.5-sonnet-20240620',
        name='Claude 3.5 Sonnet (2024-06-20)',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/anthropic/claude-3.7-sonnet": Model(
        id='anthropic/claude-3.7-sonnet',
        name='Claude 3.7 Sonnet',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=200000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-haiku-4.5": Model(
        id='anthropic/claude-haiku-4.5',
        name='Claude Haiku 4.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=5.0, cache_read=0.09999999999999999, cache_write=1.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-opus-4": Model(
        id='anthropic/claude-opus-4',
        name='Claude Opus 4',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "vercel-ai-gateway/anthropic/claude-opus-4.1": Model(
        id='anthropic/claude-opus-4.1',
        name='Claude Opus 4.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=75.0, cache_read=1.5, cache_write=18.75),
        context_window=200000,
        max_tokens=32000,
    ),
    "vercel-ai-gateway/anthropic/claude-opus-4.5": Model(
        id='anthropic/claude-opus-4.5',
        name='Claude Opus 4.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=200000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-opus-4.6": Model(
        id='anthropic/claude-opus-4.6',
        name='Claude Opus 4.6',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.5, cache_write=6.25),
        context_window=1000000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/anthropic/claude-sonnet-4": Model(
        id='anthropic/claude-sonnet-4',
        name='Claude Sonnet 4',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-sonnet-4.5": Model(
        id='anthropic/claude-sonnet-4.5',
        name='Claude Sonnet 4.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/anthropic/claude-sonnet-4.6": Model(
        id='anthropic/claude-sonnet-4.6',
        name='Claude Sonnet 4.6',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.3, cache_write=3.75),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/arcee-ai/trinity-large-preview": Model(
        id='arcee-ai/trinity-large-preview',
        name='Trinity Large Preview',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=131000,
        max_tokens=131000,
    ),
    "vercel-ai-gateway/bytedance/seed-1.6": Model(
        id='bytedance/seed-1.6',
        name='Seed 1.6',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=256000,
        max_tokens=32000,
    ),
    "vercel-ai-gateway/cohere/command-a": Model(
        id='cohere/command-a',
        name='Command A',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=8000,
    ),
    "vercel-ai-gateway/deepseek/deepseek-v3": Model(
        id='deepseek/deepseek-v3',
        name='DeepSeek V3 0324',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.77, output=0.77, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/deepseek/deepseek-v3.1": Model(
        id='deepseek/deepseek-v3.1',
        name='DeepSeek-V3.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.21, output=0.7899999999999999, cache_read=0.0, cache_write=0.0),
        context_window=163840,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/deepseek/deepseek-v3.1-terminus": Model(
        id='deepseek/deepseek-v3.1-terminus',
        name='DeepSeek V3.1 Terminus',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.27, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/deepseek/deepseek-v3.2": Model(
        id='deepseek/deepseek-v3.2',
        name='DeepSeek V3.2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.26, output=0.38, cache_read=0.13, cache_write=0.0),
        context_window=128000,
        max_tokens=8000,
    ),
    "vercel-ai-gateway/deepseek/deepseek-v3.2-thinking": Model(
        id='deepseek/deepseek-v3.2-thinking',
        name='DeepSeek V3.2 Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.28, output=0.42, cache_read=0.028, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/google/gemini-2.5-flash": Model(
        id='google/gemini-2.5-flash',
        name='Gemini 2.5 Flash',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.0, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/google/gemini-2.5-flash-lite": Model(
        id='google/gemini-2.5-flash-lite',
        name='Gemini 2.5 Flash Lite',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.01, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/google/gemini-2.5-flash-lite-preview-09-2025": Model(
        id='google/gemini-2.5-flash-lite-preview-09-2025',
        name='Gemini 2.5 Flash Lite Preview 09-2025',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.01, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/google/gemini-2.5-flash-preview-09-2025": Model(
        id='google/gemini-2.5-flash-preview-09-2025',
        name='Gemini 2.5 Flash Preview 09-2025',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=2.5, cache_read=0.03, cache_write=0.0),
        context_window=1000000,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/google/gemini-2.5-pro": Model(
        id='google/gemini-2.5-pro',
        name='Gemini 2.5 Pro',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=1048576,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/google/gemini-3-flash": Model(
        id='google/gemini-3-flash',
        name='Gemini 3 Flash',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=3.0, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/google/gemini-3-pro-preview": Model(
        id='google/gemini-3-pro-preview',
        name='Gemini 3 Pro Preview',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=12.0, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=1000000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/inception/mercury-coder-small": Model(
        id='inception/mercury-coder-small',
        name='Mercury Coder Small Beta',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.25, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/meituan/longcat-flash-chat": Model(
        id='meituan/longcat-flash-chat',
        name='LongCat Flash Chat',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meituan/longcat-flash-thinking": Model(
        id='meituan/longcat-flash-thinking',
        name='LongCat Flash Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.15, output=1.5, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meta/llama-3.1-70b": Model(
        id='meta/llama-3.1-70b',
        name='Llama 3.1 70B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.39999999999999997, output=0.39999999999999997, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/meta/llama-3.1-8b": Model(
        id='meta/llama-3.1-8b',
        name='Llama 3.1 8B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.03, output=0.049999999999999996, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/meta/llama-3.2-11b": Model(
        id='meta/llama-3.2-11b',
        name='Llama 3.2 11B Vision Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.16, output=0.16, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meta/llama-3.2-90b": Model(
        id='meta/llama-3.2-90b',
        name='Llama 3.2 90B Vision Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.72, output=0.72, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meta/llama-3.3-70b": Model(
        id='meta/llama-3.3-70b',
        name='Llama 3.3 70B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.72, output=0.72, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meta/llama-4-maverick": Model(
        id='meta/llama-4-maverick',
        name='Llama 4 Maverick 17B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/meta/llama-4-scout": Model(
        id='meta/llama-4-scout',
        name='Llama 4 Scout 17B Instruct',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.08, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/minimax/minimax-m2": Model(
        id='minimax/minimax-m2',
        name='MiniMax M2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.03, cache_write=0.375),
        context_window=205000,
        max_tokens=205000,
    ),
    "vercel-ai-gateway/minimax/minimax-m2.1": Model(
        id='minimax/minimax-m2.1',
        name='MiniMax M2.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.15, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/minimax/minimax-m2.1-lightning": Model(
        id='minimax/minimax-m2.1-lightning',
        name='MiniMax M2.1 Lightning',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=2.4, cache_read=0.03, cache_write=0.375),
        context_window=204800,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/minimax/minimax-m2.5": Model(
        id='minimax/minimax-m2.5',
        name='MiniMax M2.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=1.2, cache_read=0.03, cache_write=0.375),
        context_window=204800,
        max_tokens=131000,
    ),
    "vercel-ai-gateway/mistral/codestral": Model(
        id='mistral/codestral',
        name='Mistral Codestral',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.8999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/mistral/devstral-2": Model(
        id='mistral/devstral-2',
        name='Devstral 2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/mistral/devstral-small": Model(
        id='mistral/devstral-small',
        name='Devstral Small 1.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/mistral/devstral-small-2": Model(
        id='mistral/devstral-small-2',
        name='Devstral Small 2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/mistral/ministral-3b": Model(
        id='mistral/ministral-3b',
        name='Ministral 3B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.04, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/mistral/ministral-8b": Model(
        id='mistral/ministral-8b',
        name='Ministral 8B',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.09999999999999999, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/mistral/mistral-medium": Model(
        id='mistral/mistral-medium',
        name='Mistral Medium 3.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=64000,
    ),
    "vercel-ai-gateway/mistral/mistral-small": Model(
        id='mistral/mistral-small',
        name='Mistral Small',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=32000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/mistral/pixtral-12b": Model(
        id='mistral/pixtral-12b',
        name='Pixtral 12B 2409',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.15, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/mistral/pixtral-large": Model(
        id='mistral/pixtral-large',
        name='Pixtral Large',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=6.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4000,
    ),
    "vercel-ai-gateway/moonshotai/kimi-k2": Model(
        id='moonshotai/kimi-k2',
        name='Kimi K2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.5, output=2.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/moonshotai/kimi-k2-thinking": Model(
        id='moonshotai/kimi-k2-thinking',
        name='Kimi K2 Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.47, output=2.0, cache_read=0.14100000000000001, cache_write=0.0),
        context_window=216144,
        max_tokens=216144,
    ),
    "vercel-ai-gateway/moonshotai/kimi-k2-thinking-turbo": Model(
        id='moonshotai/kimi-k2-thinking-turbo',
        name='Kimi K2 Thinking Turbo',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.15, output=8.0, cache_read=0.15, cache_write=0.0),
        context_window=262114,
        max_tokens=262114,
    ),
    "vercel-ai-gateway/moonshotai/kimi-k2-turbo": Model(
        id='moonshotai/kimi-k2-turbo',
        name='Kimi K2 Turbo',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.4, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/moonshotai/kimi-k2.5": Model(
        id='moonshotai/kimi-k2.5',
        name='Kimi K2.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.5, output=2.8, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/nvidia/nemotron-nano-12b-v2-vl": Model(
        id='nvidia/nemotron-nano-12b-v2-vl',
        name='Nvidia Nemotron Nano 12B V2 VL',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.19999999999999998, output=0.6, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/nvidia/nemotron-nano-9b-v2": Model(
        id='nvidia/nemotron-nano-9b-v2',
        name='Nvidia Nemotron Nano 9B V2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.04, output=0.16, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/openai/codex-mini": Model(
        id='openai/codex-mini',
        name='Codex Mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.5, output=6.0, cache_read=0.375, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/gpt-4-turbo": Model(
        id='openai/gpt-4-turbo',
        name='GPT-4 Turbo',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=30.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=4096,
    ),
    "vercel-ai-gateway/openai/gpt-4.1": Model(
        id='openai/gpt-4.1',
        name='GPT-4.1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/openai/gpt-4.1-mini": Model(
        id='openai/gpt-4.1-mini',
        name='GPT-4.1 mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.39999999999999997, output=1.5999999999999999, cache_read=0.09999999999999999, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/openai/gpt-4.1-nano": Model(
        id='openai/gpt-4.1-nano',
        name='GPT-4.1 nano',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.09999999999999999, output=0.39999999999999997, cache_read=0.03, cache_write=0.0),
        context_window=1047576,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/openai/gpt-4o": Model(
        id='openai/gpt-4o',
        name='GPT-4o',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.5, output=10.0, cache_read=1.25, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/openai/gpt-4o-mini": Model(
        id='openai/gpt-4o-mini',
        name='GPT-4o mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.15, output=0.6, cache_read=0.075, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/openai/gpt-5": Model(
        id='openai/gpt-5',
        name='GPT-5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5-chat": Model(
        id='openai/gpt-5-chat',
        name='GPT-5 Chat',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/openai/gpt-5-codex": Model(
        id='openai/gpt-5-codex',
        name='GPT-5-Codex',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5-mini": Model(
        id='openai/gpt-5-mini',
        name='GPT-5 mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.03, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5-nano": Model(
        id='openai/gpt-5-nano',
        name='GPT-5 nano',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.049999999999999996, output=0.39999999999999997, cache_read=0.01, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5-pro": Model(
        id='openai/gpt-5-pro',
        name='GPT-5 pro',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=120.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=272000,
    ),
    "vercel-ai-gateway/openai/gpt-5.1-codex": Model(
        id='openai/gpt-5.1-codex',
        name='GPT-5.1-Codex',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.1-codex-max": Model(
        id='openai/gpt-5.1-codex-max',
        name='GPT 5.1 Codex Max',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.125, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.1-codex-mini": Model(
        id='openai/gpt-5.1-codex-mini',
        name='GPT-5.1 Codex mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.25, output=2.0, cache_read=0.024999999999999998, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.1-instant": Model(
        id='openai/gpt-5.1-instant',
        name='GPT-5.1 Instant',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/openai/gpt-5.1-thinking": Model(
        id='openai/gpt-5.1-thinking',
        name='GPT 5.1 Thinking',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.25, output=10.0, cache_read=0.13, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.2": Model(
        id='openai/gpt-5.2',
        name='GPT 5.2',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.18, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.2-chat": Model(
        id='openai/gpt-5.2-chat',
        name='GPT-5.2 Chat',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=128000,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/openai/gpt-5.2-codex": Model(
        id='openai/gpt-5.2-codex',
        name='GPT-5.2-Codex',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.75, output=14.0, cache_read=0.175, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-5.2-pro": Model(
        id='openai/gpt-5.2-pro',
        name='GPT 5.2 ',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=21.0, output=168.0, cache_read=0.0, cache_write=0.0),
        context_window=400000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/openai/gpt-oss-120b": Model(
        id='openai/gpt-oss-120b',
        name='gpt-oss-120b',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09999999999999999, output=0.5, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/openai/gpt-oss-20b": Model(
        id='openai/gpt-oss-20b',
        name='gpt-oss-20b',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.07, output=0.3, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=8192,
    ),
    "vercel-ai-gateway/openai/gpt-oss-safeguard-20b": Model(
        id='openai/gpt-oss-safeguard-20b',
        name='gpt-oss-safeguard-20b',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.075, output=0.3, cache_read=0.037, cache_write=0.0),
        context_window=131072,
        max_tokens=65536,
    ),
    "vercel-ai-gateway/openai/o1": Model(
        id='openai/o1',
        name='o1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=15.0, output=60.0, cache_read=7.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/o3": Model(
        id='openai/o3',
        name='o3',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=8.0, cache_read=0.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/o3-deep-research": Model(
        id='openai/o3-deep-research',
        name='o3-deep-research',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=10.0, output=40.0, cache_read=2.5, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/o3-mini": Model(
        id='openai/o3-mini',
        name='o3-mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.55, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/o3-pro": Model(
        id='openai/o3-pro',
        name='o3 Pro',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=20.0, output=80.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/openai/o4-mini": Model(
        id='openai/o4-mini',
        name='o4-mini',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=1.1, output=4.4, cache_read=0.275, cache_write=0.0),
        context_window=200000,
        max_tokens=100000,
    ),
    "vercel-ai-gateway/perplexity/sonar": Model(
        id='perplexity/sonar',
        name='Sonar',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=1.0, output=1.0, cache_read=0.0, cache_write=0.0),
        context_window=127000,
        max_tokens=8000,
    ),
    "vercel-ai-gateway/perplexity/sonar-pro": Model(
        id='perplexity/sonar-pro',
        name='Sonar Pro',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=8000,
    ),
    "vercel-ai-gateway/prime-intellect/intellect-3": Model(
        id='prime-intellect/intellect-3',
        name='INTELLECT 3',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=1.1, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/vercel/v0-1.0-md": Model(
        id='vercel/v0-1.0-md',
        name='v0-1.0-md',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32000,
    ),
    "vercel-ai-gateway/vercel/v0-1.5-md": Model(
        id='vercel/v0-1.5-md',
        name='v0-1.5-md',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/xai/grok-2-vision": Model(
        id='xai/grok-2-vision',
        name='Grok 2 Vision',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=0.0, cache_write=0.0),
        context_window=32768,
        max_tokens=32768,
    ),
    "vercel-ai-gateway/xai/grok-3": Model(
        id='xai/grok-3',
        name='Grok 3 Beta',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/xai/grok-3-fast": Model(
        id='xai/grok-3-fast',
        name='Grok 3 Fast Beta',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/xai/grok-3-mini": Model(
        id='xai/grok-3-mini',
        name='Grok 3 Mini Beta',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/xai/grok-3-mini-fast": Model(
        id='xai/grok-3-mini-fast',
        name='Grok 3 Mini Fast Beta',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.6, output=4.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/xai/grok-4": Model(
        id='xai/grok-4',
        name='Grok 4',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.0, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/xai/grok-4-fast-non-reasoning": Model(
        id='xai/grok-4-fast-non-reasoning',
        name='Grok 4 Fast Non-Reasoning',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/xai/grok-4-fast-reasoning": Model(
        id='xai/grok-4-fast-reasoning',
        name='Grok 4 Fast Reasoning',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/xai/grok-4.1-fast-non-reasoning": Model(
        id='xai/grok-4.1-fast-non-reasoning',
        name='Grok 4.1 Fast Non-Reasoning',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "vercel-ai-gateway/xai/grok-4.1-fast-reasoning": Model(
        id='xai/grok-4.1-fast-reasoning',
        name='Grok 4.1 Fast Reasoning',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=0.5, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "vercel-ai-gateway/xai/grok-code-fast-1": Model(
        id='xai/grok-code-fast-1',
        name='Grok Code Fast 1',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=1.5, cache_read=0.02, cache_write=0.0),
        context_window=256000,
        max_tokens=256000,
    ),
    "vercel-ai-gateway/xiaomi/mimo-v2-flash": Model(
        id='xiaomi/mimo-v2-flash',
        name='MiMo V2 Flash',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.09, output=0.29, cache_read=0.0, cache_write=0.0),
        context_window=262144,
        max_tokens=32000,
    ),
    "vercel-ai-gateway/zai/glm-4.5": Model(
        id='zai/glm-4.5',
        name='GLM-4.5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=131072,
    ),
    "vercel-ai-gateway/zai/glm-4.5-air": Model(
        id='zai/glm-4.5-air',
        name='GLM 4.5 Air',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.19999999999999998, output=1.1, cache_read=0.03, cache_write=0.0),
        context_window=128000,
        max_tokens=96000,
    ),
    "vercel-ai-gateway/zai/glm-4.5v": Model(
        id='zai/glm-4.5v',
        name='GLM 4.5V',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=1.7999999999999998, cache_read=0.0, cache_write=0.0),
        context_window=65536,
        max_tokens=16384,
    ),
    "vercel-ai-gateway/zai/glm-4.6": Model(
        id='zai/glm-4.6',
        name='GLM 4.6',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.44999999999999996, output=1.7999999999999998, cache_read=0.11, cache_write=0.0),
        context_window=200000,
        max_tokens=96000,
    ),
    "vercel-ai-gateway/zai/glm-4.6v": Model(
        id='zai/glm-4.6v',
        name='GLM-4.6V',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=0.8999999999999999, cache_read=0.049999999999999996, cache_write=0.0),
        context_window=128000,
        max_tokens=24000,
    ),
    "vercel-ai-gateway/zai/glm-4.6v-flash": Model(
        id='zai/glm-4.6v-flash',
        name='GLM-4.6V-Flash',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=24000,
    ),
    "vercel-ai-gateway/zai/glm-4.7": Model(
        id='zai/glm-4.7',
        name='GLM 4.7',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.43, output=1.75, cache_read=0.08, cache_write=0.0),
        context_window=202752,
        max_tokens=120000,
    ),
    "vercel-ai-gateway/zai/glm-4.7-flashx": Model(
        id='zai/glm-4.7-flashx',
        name='GLM 4.7 FlashX',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.06, output=0.39999999999999997, cache_read=0.01, cache_write=0.0),
        context_window=200000,
        max_tokens=128000,
    ),
    "vercel-ai-gateway/zai/glm-5": Model(
        id='zai/glm-5',
        name='GLM-5',
        api='anthropic-messages',
        provider='vercel-ai-gateway',
        base_url='https://ai-gateway.vercel.sh',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.1999999999999997, cache_read=0.19999999999999998, cache_write=0.0),
        context_window=202800,
        max_tokens=131072,
    ),
    "xai/grok-2": Model(
        id='grok-2',
        name='Grok 2',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-2-1212": Model(
        id='grok-2-1212',
        name='Grok 2 (1212)',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-2-latest": Model(
        id='grok-2-latest',
        name='Grok 2 Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-2-vision": Model(
        id='grok-2-vision',
        name='Grok 2 Vision',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=8192,
        max_tokens=4096,
    ),
    "xai/grok-2-vision-1212": Model(
        id='grok-2-vision-1212',
        name='Grok 2 Vision (1212)',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=8192,
        max_tokens=4096,
    ),
    "xai/grok-2-vision-latest": Model(
        id='grok-2-vision-latest',
        name='Grok 2 Vision Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=2.0, output=10.0, cache_read=2.0, cache_write=0.0),
        context_window=8192,
        max_tokens=4096,
    ),
    "xai/grok-3": Model(
        id='grok-3',
        name='Grok 3',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-fast": Model(
        id='grok-3-fast',
        name='Grok 3 Fast',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=1.25, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-fast-latest": Model(
        id='grok-3-fast-latest',
        name='Grok 3 Fast Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=5.0, output=25.0, cache_read=1.25, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-latest": Model(
        id='grok-3-latest',
        name='Grok 3 Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-mini": Model(
        id='grok-3-mini',
        name='Grok 3 Mini',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-mini-fast": Model(
        id='grok-3-mini-fast',
        name='Grok 3 Mini Fast',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=4.0, cache_read=0.15, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-mini-fast-latest": Model(
        id='grok-3-mini-fast-latest',
        name='Grok 3 Mini Fast Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=4.0, cache_read=0.15, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-3-mini-latest": Model(
        id='grok-3-mini-latest',
        name='Grok 3 Mini Latest',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.3, output=0.5, cache_read=0.075, cache_write=0.0),
        context_window=131072,
        max_tokens=8192,
    ),
    "xai/grok-4": Model(
        id='grok-4',
        name='Grok 4',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=3.0, output=15.0, cache_read=0.75, cache_write=0.0),
        context_window=256000,
        max_tokens=64000,
    ),
    "xai/grok-4-1-fast": Model(
        id='grok-4-1-fast',
        name='Grok 4.1 Fast',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "xai/grok-4-1-fast-non-reasoning": Model(
        id='grok-4-1-fast-non-reasoning',
        name='Grok 4.1 Fast (Non-Reasoning)',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "xai/grok-4-fast": Model(
        id='grok-4-fast',
        name='Grok 4 Fast',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "xai/grok-4-fast-non-reasoning": Model(
        id='grok-4-fast-non-reasoning',
        name='Grok 4 Fast (Non-Reasoning)',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=0.2, output=0.5, cache_read=0.05, cache_write=0.0),
        context_window=2000000,
        max_tokens=30000,
    ),
    "xai/grok-beta": Model(
        id='grok-beta',
        name='Grok Beta',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text"],
        cost=ModelCost(input=5.0, output=15.0, cache_read=5.0, cache_write=0.0),
        context_window=131072,
        max_tokens=4096,
    ),
    "xai/grok-code-fast-1": Model(
        id='grok-code-fast-1',
        name='Grok Code Fast 1',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.2, output=1.5, cache_read=0.02, cache_write=0.0),
        context_window=256000,
        max_tokens=10000,
    ),
    "xai/grok-vision-beta": Model(
        id='grok-vision-beta',
        name='Grok Vision Beta',
        api='openai-completions',
        provider='xai',
        base_url='https://api.x.ai/v1',
        reasoning=False,
        input=["text", "image"],
        cost=ModelCost(input=5.0, output=15.0, cache_read=5.0, cache_write=0.0),
        context_window=8192,
        max_tokens=4096,
    ),
    "zai/glm-4.5": Model(
        id='glm-4.5',
        name='GLM-4.5',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0.0),
        context_window=131072,
        max_tokens=98304,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.5-air": Model(
        id='glm-4.5-air',
        name='GLM-4.5-Air',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.2, output=1.1, cache_read=0.03, cache_write=0.0),
        context_window=131072,
        max_tokens=98304,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.5-flash": Model(
        id='glm-4.5-flash',
        name='GLM-4.5-Flash',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=131072,
        max_tokens=98304,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.5v": Model(
        id='glm-4.5v',
        name='GLM-4.5V',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.6, output=1.8, cache_read=0.0, cache_write=0.0),
        context_window=64000,
        max_tokens=16384,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.6": Model(
        id='glm-4.6',
        name='GLM-4.6',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.6v": Model(
        id='glm-4.6v',
        name='GLM-4.6V',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text", "image"],
        cost=ModelCost(input=0.3, output=0.9, cache_read=0.0, cache_write=0.0),
        context_window=128000,
        max_tokens=32768,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.7": Model(
        id='glm-4.7',
        name='GLM-4.7',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.6, output=2.2, cache_read=0.11, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-4.7-flash": Model(
        id='glm-4.7-flash',
        name='GLM-4.7-Flash',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=0.0, output=0.0, cache_read=0.0, cache_write=0.0),
        context_window=200000,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
    "zai/glm-5": Model(
        id='glm-5',
        name='GLM-5',
        api='openai-completions',
        provider='zai',
        base_url='https://api.z.ai/api/coding/paas/v4',
        reasoning=True,
        input=["text"],
        cost=ModelCost(input=1.0, output=3.2, cache_read=0.2, cache_write=0.0),
        context_window=204800,
        max_tokens=131072,
        compat={'supportsDeveloperRole': False, 'thinkingFormat': 'zai'},
    ),
}
